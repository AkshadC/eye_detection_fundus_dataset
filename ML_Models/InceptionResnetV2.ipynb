{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:16:48.488415Z",
     "start_time": "2024-07-26T01:16:48.485824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import utils\n",
    "from tensorflow.keras import mixed_precision\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:16:48.674741Z",
     "start_time": "2024-07-26T01:16:48.672372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "#Batching using prefetch\n",
    "train_data_casted = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "test_data_casted = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\"\"\""
   ],
   "id": "2bfc02fd3bd68fe0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Batching using prefetch\\ntrain_data_casted = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE)\\ntest_data_casted = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:16:58.195617Z",
     "start_time": "2024-07-26T01:16:58.193737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fundus_train = \"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/Dataset/split1/train\"\n",
    "fundus_test = \"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/Dataset/split1/test\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n"
   ],
   "id": "76e3a5421f09f74d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:16:58.520314Z",
     "start_time": "2024-07-26T01:16:58.518228Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.listdir(fundus_train))",
   "id": "1a2371451891c6ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glaucoma', 'normal', 'cataract', 'diabetic_retinopathy']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:16:58.967667Z",
     "start_time": "2024-07-26T01:16:58.965631Z"
    }
   },
   "cell_type": "code",
   "source": "IMG_HEIGHT, IMG_WIDTH = 299, 299",
   "id": "1f94cba87987258",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:00.188326Z",
     "start_time": "2024-07-26T01:16:59.282770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_train,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_train,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "    validation_split=0.3,\n",
    "    subset='validation'\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_test,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "\n",
    ")"
   ],
   "id": "348e18c639023b17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3372 files belonging to 4 classes.\n",
      "Using 2361 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:16:59.344346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-25 21:16:59.349453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-25 21:16:59.353176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-25 21:16:59.473912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-25 21:16:59.475794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-25 21:16:59.477302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-25 21:16:59.478714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5964 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-07-25 21:16:59.620256: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3372 files belonging to 4 classes.\n",
      "Using 1011 files for validation.\n",
      "Found 845 files belonging to 4 classes.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:00.192068Z",
     "start_time": "2024-07-26T01:17:00.189152Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset",
   "id": "d09dab585b1ca859",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:00.203100Z",
     "start_time": "2024-07-26T01:17:00.193435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Using tf prefetch dataset\n",
    "preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input"
   ],
   "id": "c66d836742f62b76",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:01.649900Z",
     "start_time": "2024-07-26T01:17:01.616052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datagen = train_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "val_datagen = validation_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "test_datagen = test_dataset.map(lambda x, y: (preprocess_input(x), y))"
   ],
   "id": "fe651f3421b86187",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:02.287363Z",
     "start_time": "2024-07-26T01:17:02.284708Z"
    }
   },
   "cell_type": "code",
   "source": "train_datagen",
   "id": "2d604d0924419863",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 299, 299, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:06.261630Z",
     "start_time": "2024-07-26T01:17:03.521609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow.keras.applications as apps\n",
    "base_model = apps.InceptionResNetV2(weights = 'imagenet', include_top = False, input_shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.trainable = False"
   ],
   "id": "541ea676b52829f7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:07.103989Z",
     "start_time": "2024-07-26T01:17:07.098590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for i in base_model.layers:\n",
    "    print(f'Layer: {i.name}, {i.trainable}')"
   ],
   "id": "79db08db0282846b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_1, False\n",
      "Layer: conv2d, False\n",
      "Layer: batch_normalization, False\n",
      "Layer: activation, False\n",
      "Layer: conv2d_1, False\n",
      "Layer: batch_normalization_1, False\n",
      "Layer: activation_1, False\n",
      "Layer: conv2d_2, False\n",
      "Layer: batch_normalization_2, False\n",
      "Layer: activation_2, False\n",
      "Layer: max_pooling2d, False\n",
      "Layer: conv2d_3, False\n",
      "Layer: batch_normalization_3, False\n",
      "Layer: activation_3, False\n",
      "Layer: conv2d_4, False\n",
      "Layer: batch_normalization_4, False\n",
      "Layer: activation_4, False\n",
      "Layer: max_pooling2d_1, False\n",
      "Layer: conv2d_8, False\n",
      "Layer: batch_normalization_8, False\n",
      "Layer: activation_8, False\n",
      "Layer: conv2d_6, False\n",
      "Layer: conv2d_9, False\n",
      "Layer: batch_normalization_6, False\n",
      "Layer: batch_normalization_9, False\n",
      "Layer: activation_6, False\n",
      "Layer: activation_9, False\n",
      "Layer: average_pooling2d, False\n",
      "Layer: conv2d_5, False\n",
      "Layer: conv2d_7, False\n",
      "Layer: conv2d_10, False\n",
      "Layer: conv2d_11, False\n",
      "Layer: batch_normalization_5, False\n",
      "Layer: batch_normalization_7, False\n",
      "Layer: batch_normalization_10, False\n",
      "Layer: batch_normalization_11, False\n",
      "Layer: activation_5, False\n",
      "Layer: activation_7, False\n",
      "Layer: activation_10, False\n",
      "Layer: activation_11, False\n",
      "Layer: mixed_5b, False\n",
      "Layer: conv2d_15, False\n",
      "Layer: batch_normalization_15, False\n",
      "Layer: activation_15, False\n",
      "Layer: conv2d_13, False\n",
      "Layer: conv2d_16, False\n",
      "Layer: batch_normalization_13, False\n",
      "Layer: batch_normalization_16, False\n",
      "Layer: activation_13, False\n",
      "Layer: activation_16, False\n",
      "Layer: conv2d_12, False\n",
      "Layer: conv2d_14, False\n",
      "Layer: conv2d_17, False\n",
      "Layer: batch_normalization_12, False\n",
      "Layer: batch_normalization_14, False\n",
      "Layer: batch_normalization_17, False\n",
      "Layer: activation_12, False\n",
      "Layer: activation_14, False\n",
      "Layer: activation_17, False\n",
      "Layer: block35_1_mixed, False\n",
      "Layer: block35_1_conv, False\n",
      "Layer: custom_scale_layer, False\n",
      "Layer: block35_1_ac, False\n",
      "Layer: conv2d_21, False\n",
      "Layer: batch_normalization_21, False\n",
      "Layer: activation_21, False\n",
      "Layer: conv2d_19, False\n",
      "Layer: conv2d_22, False\n",
      "Layer: batch_normalization_19, False\n",
      "Layer: batch_normalization_22, False\n",
      "Layer: activation_19, False\n",
      "Layer: activation_22, False\n",
      "Layer: conv2d_18, False\n",
      "Layer: conv2d_20, False\n",
      "Layer: conv2d_23, False\n",
      "Layer: batch_normalization_18, False\n",
      "Layer: batch_normalization_20, False\n",
      "Layer: batch_normalization_23, False\n",
      "Layer: activation_18, False\n",
      "Layer: activation_20, False\n",
      "Layer: activation_23, False\n",
      "Layer: block35_2_mixed, False\n",
      "Layer: block35_2_conv, False\n",
      "Layer: custom_scale_layer_1, False\n",
      "Layer: block35_2_ac, False\n",
      "Layer: conv2d_27, False\n",
      "Layer: batch_normalization_27, False\n",
      "Layer: activation_27, False\n",
      "Layer: conv2d_25, False\n",
      "Layer: conv2d_28, False\n",
      "Layer: batch_normalization_25, False\n",
      "Layer: batch_normalization_28, False\n",
      "Layer: activation_25, False\n",
      "Layer: activation_28, False\n",
      "Layer: conv2d_24, False\n",
      "Layer: conv2d_26, False\n",
      "Layer: conv2d_29, False\n",
      "Layer: batch_normalization_24, False\n",
      "Layer: batch_normalization_26, False\n",
      "Layer: batch_normalization_29, False\n",
      "Layer: activation_24, False\n",
      "Layer: activation_26, False\n",
      "Layer: activation_29, False\n",
      "Layer: block35_3_mixed, False\n",
      "Layer: block35_3_conv, False\n",
      "Layer: custom_scale_layer_2, False\n",
      "Layer: block35_3_ac, False\n",
      "Layer: conv2d_33, False\n",
      "Layer: batch_normalization_33, False\n",
      "Layer: activation_33, False\n",
      "Layer: conv2d_31, False\n",
      "Layer: conv2d_34, False\n",
      "Layer: batch_normalization_31, False\n",
      "Layer: batch_normalization_34, False\n",
      "Layer: activation_31, False\n",
      "Layer: activation_34, False\n",
      "Layer: conv2d_30, False\n",
      "Layer: conv2d_32, False\n",
      "Layer: conv2d_35, False\n",
      "Layer: batch_normalization_30, False\n",
      "Layer: batch_normalization_32, False\n",
      "Layer: batch_normalization_35, False\n",
      "Layer: activation_30, False\n",
      "Layer: activation_32, False\n",
      "Layer: activation_35, False\n",
      "Layer: block35_4_mixed, False\n",
      "Layer: block35_4_conv, False\n",
      "Layer: custom_scale_layer_3, False\n",
      "Layer: block35_4_ac, False\n",
      "Layer: conv2d_39, False\n",
      "Layer: batch_normalization_39, False\n",
      "Layer: activation_39, False\n",
      "Layer: conv2d_37, False\n",
      "Layer: conv2d_40, False\n",
      "Layer: batch_normalization_37, False\n",
      "Layer: batch_normalization_40, False\n",
      "Layer: activation_37, False\n",
      "Layer: activation_40, False\n",
      "Layer: conv2d_36, False\n",
      "Layer: conv2d_38, False\n",
      "Layer: conv2d_41, False\n",
      "Layer: batch_normalization_36, False\n",
      "Layer: batch_normalization_38, False\n",
      "Layer: batch_normalization_41, False\n",
      "Layer: activation_36, False\n",
      "Layer: activation_38, False\n",
      "Layer: activation_41, False\n",
      "Layer: block35_5_mixed, False\n",
      "Layer: block35_5_conv, False\n",
      "Layer: custom_scale_layer_4, False\n",
      "Layer: block35_5_ac, False\n",
      "Layer: conv2d_45, False\n",
      "Layer: batch_normalization_45, False\n",
      "Layer: activation_45, False\n",
      "Layer: conv2d_43, False\n",
      "Layer: conv2d_46, False\n",
      "Layer: batch_normalization_43, False\n",
      "Layer: batch_normalization_46, False\n",
      "Layer: activation_43, False\n",
      "Layer: activation_46, False\n",
      "Layer: conv2d_42, False\n",
      "Layer: conv2d_44, False\n",
      "Layer: conv2d_47, False\n",
      "Layer: batch_normalization_42, False\n",
      "Layer: batch_normalization_44, False\n",
      "Layer: batch_normalization_47, False\n",
      "Layer: activation_42, False\n",
      "Layer: activation_44, False\n",
      "Layer: activation_47, False\n",
      "Layer: block35_6_mixed, False\n",
      "Layer: block35_6_conv, False\n",
      "Layer: custom_scale_layer_5, False\n",
      "Layer: block35_6_ac, False\n",
      "Layer: conv2d_51, False\n",
      "Layer: batch_normalization_51, False\n",
      "Layer: activation_51, False\n",
      "Layer: conv2d_49, False\n",
      "Layer: conv2d_52, False\n",
      "Layer: batch_normalization_49, False\n",
      "Layer: batch_normalization_52, False\n",
      "Layer: activation_49, False\n",
      "Layer: activation_52, False\n",
      "Layer: conv2d_48, False\n",
      "Layer: conv2d_50, False\n",
      "Layer: conv2d_53, False\n",
      "Layer: batch_normalization_48, False\n",
      "Layer: batch_normalization_50, False\n",
      "Layer: batch_normalization_53, False\n",
      "Layer: activation_48, False\n",
      "Layer: activation_50, False\n",
      "Layer: activation_53, False\n",
      "Layer: block35_7_mixed, False\n",
      "Layer: block35_7_conv, False\n",
      "Layer: custom_scale_layer_6, False\n",
      "Layer: block35_7_ac, False\n",
      "Layer: conv2d_57, False\n",
      "Layer: batch_normalization_57, False\n",
      "Layer: activation_57, False\n",
      "Layer: conv2d_55, False\n",
      "Layer: conv2d_58, False\n",
      "Layer: batch_normalization_55, False\n",
      "Layer: batch_normalization_58, False\n",
      "Layer: activation_55, False\n",
      "Layer: activation_58, False\n",
      "Layer: conv2d_54, False\n",
      "Layer: conv2d_56, False\n",
      "Layer: conv2d_59, False\n",
      "Layer: batch_normalization_54, False\n",
      "Layer: batch_normalization_56, False\n",
      "Layer: batch_normalization_59, False\n",
      "Layer: activation_54, False\n",
      "Layer: activation_56, False\n",
      "Layer: activation_59, False\n",
      "Layer: block35_8_mixed, False\n",
      "Layer: block35_8_conv, False\n",
      "Layer: custom_scale_layer_7, False\n",
      "Layer: block35_8_ac, False\n",
      "Layer: conv2d_63, False\n",
      "Layer: batch_normalization_63, False\n",
      "Layer: activation_63, False\n",
      "Layer: conv2d_61, False\n",
      "Layer: conv2d_64, False\n",
      "Layer: batch_normalization_61, False\n",
      "Layer: batch_normalization_64, False\n",
      "Layer: activation_61, False\n",
      "Layer: activation_64, False\n",
      "Layer: conv2d_60, False\n",
      "Layer: conv2d_62, False\n",
      "Layer: conv2d_65, False\n",
      "Layer: batch_normalization_60, False\n",
      "Layer: batch_normalization_62, False\n",
      "Layer: batch_normalization_65, False\n",
      "Layer: activation_60, False\n",
      "Layer: activation_62, False\n",
      "Layer: activation_65, False\n",
      "Layer: block35_9_mixed, False\n",
      "Layer: block35_9_conv, False\n",
      "Layer: custom_scale_layer_8, False\n",
      "Layer: block35_9_ac, False\n",
      "Layer: conv2d_69, False\n",
      "Layer: batch_normalization_69, False\n",
      "Layer: activation_69, False\n",
      "Layer: conv2d_67, False\n",
      "Layer: conv2d_70, False\n",
      "Layer: batch_normalization_67, False\n",
      "Layer: batch_normalization_70, False\n",
      "Layer: activation_67, False\n",
      "Layer: activation_70, False\n",
      "Layer: conv2d_66, False\n",
      "Layer: conv2d_68, False\n",
      "Layer: conv2d_71, False\n",
      "Layer: batch_normalization_66, False\n",
      "Layer: batch_normalization_68, False\n",
      "Layer: batch_normalization_71, False\n",
      "Layer: activation_66, False\n",
      "Layer: activation_68, False\n",
      "Layer: activation_71, False\n",
      "Layer: block35_10_mixed, False\n",
      "Layer: block35_10_conv, False\n",
      "Layer: custom_scale_layer_9, False\n",
      "Layer: block35_10_ac, False\n",
      "Layer: conv2d_73, False\n",
      "Layer: batch_normalization_73, False\n",
      "Layer: activation_73, False\n",
      "Layer: conv2d_74, False\n",
      "Layer: batch_normalization_74, False\n",
      "Layer: activation_74, False\n",
      "Layer: conv2d_72, False\n",
      "Layer: conv2d_75, False\n",
      "Layer: batch_normalization_72, False\n",
      "Layer: batch_normalization_75, False\n",
      "Layer: activation_72, False\n",
      "Layer: activation_75, False\n",
      "Layer: max_pooling2d_2, False\n",
      "Layer: mixed_6a, False\n",
      "Layer: conv2d_77, False\n",
      "Layer: batch_normalization_77, False\n",
      "Layer: activation_77, False\n",
      "Layer: conv2d_78, False\n",
      "Layer: batch_normalization_78, False\n",
      "Layer: activation_78, False\n",
      "Layer: conv2d_76, False\n",
      "Layer: conv2d_79, False\n",
      "Layer: batch_normalization_76, False\n",
      "Layer: batch_normalization_79, False\n",
      "Layer: activation_76, False\n",
      "Layer: activation_79, False\n",
      "Layer: block17_1_mixed, False\n",
      "Layer: block17_1_conv, False\n",
      "Layer: custom_scale_layer_10, False\n",
      "Layer: block17_1_ac, False\n",
      "Layer: conv2d_81, False\n",
      "Layer: batch_normalization_81, False\n",
      "Layer: activation_81, False\n",
      "Layer: conv2d_82, False\n",
      "Layer: batch_normalization_82, False\n",
      "Layer: activation_82, False\n",
      "Layer: conv2d_80, False\n",
      "Layer: conv2d_83, False\n",
      "Layer: batch_normalization_80, False\n",
      "Layer: batch_normalization_83, False\n",
      "Layer: activation_80, False\n",
      "Layer: activation_83, False\n",
      "Layer: block17_2_mixed, False\n",
      "Layer: block17_2_conv, False\n",
      "Layer: custom_scale_layer_11, False\n",
      "Layer: block17_2_ac, False\n",
      "Layer: conv2d_85, False\n",
      "Layer: batch_normalization_85, False\n",
      "Layer: activation_85, False\n",
      "Layer: conv2d_86, False\n",
      "Layer: batch_normalization_86, False\n",
      "Layer: activation_86, False\n",
      "Layer: conv2d_84, False\n",
      "Layer: conv2d_87, False\n",
      "Layer: batch_normalization_84, False\n",
      "Layer: batch_normalization_87, False\n",
      "Layer: activation_84, False\n",
      "Layer: activation_87, False\n",
      "Layer: block17_3_mixed, False\n",
      "Layer: block17_3_conv, False\n",
      "Layer: custom_scale_layer_12, False\n",
      "Layer: block17_3_ac, False\n",
      "Layer: conv2d_89, False\n",
      "Layer: batch_normalization_89, False\n",
      "Layer: activation_89, False\n",
      "Layer: conv2d_90, False\n",
      "Layer: batch_normalization_90, False\n",
      "Layer: activation_90, False\n",
      "Layer: conv2d_88, False\n",
      "Layer: conv2d_91, False\n",
      "Layer: batch_normalization_88, False\n",
      "Layer: batch_normalization_91, False\n",
      "Layer: activation_88, False\n",
      "Layer: activation_91, False\n",
      "Layer: block17_4_mixed, False\n",
      "Layer: block17_4_conv, False\n",
      "Layer: custom_scale_layer_13, False\n",
      "Layer: block17_4_ac, False\n",
      "Layer: conv2d_93, False\n",
      "Layer: batch_normalization_93, False\n",
      "Layer: activation_93, False\n",
      "Layer: conv2d_94, False\n",
      "Layer: batch_normalization_94, False\n",
      "Layer: activation_94, False\n",
      "Layer: conv2d_92, False\n",
      "Layer: conv2d_95, False\n",
      "Layer: batch_normalization_92, False\n",
      "Layer: batch_normalization_95, False\n",
      "Layer: activation_92, False\n",
      "Layer: activation_95, False\n",
      "Layer: block17_5_mixed, False\n",
      "Layer: block17_5_conv, False\n",
      "Layer: custom_scale_layer_14, False\n",
      "Layer: block17_5_ac, False\n",
      "Layer: conv2d_97, False\n",
      "Layer: batch_normalization_97, False\n",
      "Layer: activation_97, False\n",
      "Layer: conv2d_98, False\n",
      "Layer: batch_normalization_98, False\n",
      "Layer: activation_98, False\n",
      "Layer: conv2d_96, False\n",
      "Layer: conv2d_99, False\n",
      "Layer: batch_normalization_96, False\n",
      "Layer: batch_normalization_99, False\n",
      "Layer: activation_96, False\n",
      "Layer: activation_99, False\n",
      "Layer: block17_6_mixed, False\n",
      "Layer: block17_6_conv, False\n",
      "Layer: custom_scale_layer_15, False\n",
      "Layer: block17_6_ac, False\n",
      "Layer: conv2d_101, False\n",
      "Layer: batch_normalization_101, False\n",
      "Layer: activation_101, False\n",
      "Layer: conv2d_102, False\n",
      "Layer: batch_normalization_102, False\n",
      "Layer: activation_102, False\n",
      "Layer: conv2d_100, False\n",
      "Layer: conv2d_103, False\n",
      "Layer: batch_normalization_100, False\n",
      "Layer: batch_normalization_103, False\n",
      "Layer: activation_100, False\n",
      "Layer: activation_103, False\n",
      "Layer: block17_7_mixed, False\n",
      "Layer: block17_7_conv, False\n",
      "Layer: custom_scale_layer_16, False\n",
      "Layer: block17_7_ac, False\n",
      "Layer: conv2d_105, False\n",
      "Layer: batch_normalization_105, False\n",
      "Layer: activation_105, False\n",
      "Layer: conv2d_106, False\n",
      "Layer: batch_normalization_106, False\n",
      "Layer: activation_106, False\n",
      "Layer: conv2d_104, False\n",
      "Layer: conv2d_107, False\n",
      "Layer: batch_normalization_104, False\n",
      "Layer: batch_normalization_107, False\n",
      "Layer: activation_104, False\n",
      "Layer: activation_107, False\n",
      "Layer: block17_8_mixed, False\n",
      "Layer: block17_8_conv, False\n",
      "Layer: custom_scale_layer_17, False\n",
      "Layer: block17_8_ac, False\n",
      "Layer: conv2d_109, False\n",
      "Layer: batch_normalization_109, False\n",
      "Layer: activation_109, False\n",
      "Layer: conv2d_110, False\n",
      "Layer: batch_normalization_110, False\n",
      "Layer: activation_110, False\n",
      "Layer: conv2d_108, False\n",
      "Layer: conv2d_111, False\n",
      "Layer: batch_normalization_108, False\n",
      "Layer: batch_normalization_111, False\n",
      "Layer: activation_108, False\n",
      "Layer: activation_111, False\n",
      "Layer: block17_9_mixed, False\n",
      "Layer: block17_9_conv, False\n",
      "Layer: custom_scale_layer_18, False\n",
      "Layer: block17_9_ac, False\n",
      "Layer: conv2d_113, False\n",
      "Layer: batch_normalization_113, False\n",
      "Layer: activation_113, False\n",
      "Layer: conv2d_114, False\n",
      "Layer: batch_normalization_114, False\n",
      "Layer: activation_114, False\n",
      "Layer: conv2d_112, False\n",
      "Layer: conv2d_115, False\n",
      "Layer: batch_normalization_112, False\n",
      "Layer: batch_normalization_115, False\n",
      "Layer: activation_112, False\n",
      "Layer: activation_115, False\n",
      "Layer: block17_10_mixed, False\n",
      "Layer: block17_10_conv, False\n",
      "Layer: custom_scale_layer_19, False\n",
      "Layer: block17_10_ac, False\n",
      "Layer: conv2d_117, False\n",
      "Layer: batch_normalization_117, False\n",
      "Layer: activation_117, False\n",
      "Layer: conv2d_118, False\n",
      "Layer: batch_normalization_118, False\n",
      "Layer: activation_118, False\n",
      "Layer: conv2d_116, False\n",
      "Layer: conv2d_119, False\n",
      "Layer: batch_normalization_116, False\n",
      "Layer: batch_normalization_119, False\n",
      "Layer: activation_116, False\n",
      "Layer: activation_119, False\n",
      "Layer: block17_11_mixed, False\n",
      "Layer: block17_11_conv, False\n",
      "Layer: custom_scale_layer_20, False\n",
      "Layer: block17_11_ac, False\n",
      "Layer: conv2d_121, False\n",
      "Layer: batch_normalization_121, False\n",
      "Layer: activation_121, False\n",
      "Layer: conv2d_122, False\n",
      "Layer: batch_normalization_122, False\n",
      "Layer: activation_122, False\n",
      "Layer: conv2d_120, False\n",
      "Layer: conv2d_123, False\n",
      "Layer: batch_normalization_120, False\n",
      "Layer: batch_normalization_123, False\n",
      "Layer: activation_120, False\n",
      "Layer: activation_123, False\n",
      "Layer: block17_12_mixed, False\n",
      "Layer: block17_12_conv, False\n",
      "Layer: custom_scale_layer_21, False\n",
      "Layer: block17_12_ac, False\n",
      "Layer: conv2d_125, False\n",
      "Layer: batch_normalization_125, False\n",
      "Layer: activation_125, False\n",
      "Layer: conv2d_126, False\n",
      "Layer: batch_normalization_126, False\n",
      "Layer: activation_126, False\n",
      "Layer: conv2d_124, False\n",
      "Layer: conv2d_127, False\n",
      "Layer: batch_normalization_124, False\n",
      "Layer: batch_normalization_127, False\n",
      "Layer: activation_124, False\n",
      "Layer: activation_127, False\n",
      "Layer: block17_13_mixed, False\n",
      "Layer: block17_13_conv, False\n",
      "Layer: custom_scale_layer_22, False\n",
      "Layer: block17_13_ac, False\n",
      "Layer: conv2d_129, False\n",
      "Layer: batch_normalization_129, False\n",
      "Layer: activation_129, False\n",
      "Layer: conv2d_130, False\n",
      "Layer: batch_normalization_130, False\n",
      "Layer: activation_130, False\n",
      "Layer: conv2d_128, False\n",
      "Layer: conv2d_131, False\n",
      "Layer: batch_normalization_128, False\n",
      "Layer: batch_normalization_131, False\n",
      "Layer: activation_128, False\n",
      "Layer: activation_131, False\n",
      "Layer: block17_14_mixed, False\n",
      "Layer: block17_14_conv, False\n",
      "Layer: custom_scale_layer_23, False\n",
      "Layer: block17_14_ac, False\n",
      "Layer: conv2d_133, False\n",
      "Layer: batch_normalization_133, False\n",
      "Layer: activation_133, False\n",
      "Layer: conv2d_134, False\n",
      "Layer: batch_normalization_134, False\n",
      "Layer: activation_134, False\n",
      "Layer: conv2d_132, False\n",
      "Layer: conv2d_135, False\n",
      "Layer: batch_normalization_132, False\n",
      "Layer: batch_normalization_135, False\n",
      "Layer: activation_132, False\n",
      "Layer: activation_135, False\n",
      "Layer: block17_15_mixed, False\n",
      "Layer: block17_15_conv, False\n",
      "Layer: custom_scale_layer_24, False\n",
      "Layer: block17_15_ac, False\n",
      "Layer: conv2d_137, False\n",
      "Layer: batch_normalization_137, False\n",
      "Layer: activation_137, False\n",
      "Layer: conv2d_138, False\n",
      "Layer: batch_normalization_138, False\n",
      "Layer: activation_138, False\n",
      "Layer: conv2d_136, False\n",
      "Layer: conv2d_139, False\n",
      "Layer: batch_normalization_136, False\n",
      "Layer: batch_normalization_139, False\n",
      "Layer: activation_136, False\n",
      "Layer: activation_139, False\n",
      "Layer: block17_16_mixed, False\n",
      "Layer: block17_16_conv, False\n",
      "Layer: custom_scale_layer_25, False\n",
      "Layer: block17_16_ac, False\n",
      "Layer: conv2d_141, False\n",
      "Layer: batch_normalization_141, False\n",
      "Layer: activation_141, False\n",
      "Layer: conv2d_142, False\n",
      "Layer: batch_normalization_142, False\n",
      "Layer: activation_142, False\n",
      "Layer: conv2d_140, False\n",
      "Layer: conv2d_143, False\n",
      "Layer: batch_normalization_140, False\n",
      "Layer: batch_normalization_143, False\n",
      "Layer: activation_140, False\n",
      "Layer: activation_143, False\n",
      "Layer: block17_17_mixed, False\n",
      "Layer: block17_17_conv, False\n",
      "Layer: custom_scale_layer_26, False\n",
      "Layer: block17_17_ac, False\n",
      "Layer: conv2d_145, False\n",
      "Layer: batch_normalization_145, False\n",
      "Layer: activation_145, False\n",
      "Layer: conv2d_146, False\n",
      "Layer: batch_normalization_146, False\n",
      "Layer: activation_146, False\n",
      "Layer: conv2d_144, False\n",
      "Layer: conv2d_147, False\n",
      "Layer: batch_normalization_144, False\n",
      "Layer: batch_normalization_147, False\n",
      "Layer: activation_144, False\n",
      "Layer: activation_147, False\n",
      "Layer: block17_18_mixed, False\n",
      "Layer: block17_18_conv, False\n",
      "Layer: custom_scale_layer_27, False\n",
      "Layer: block17_18_ac, False\n",
      "Layer: conv2d_149, False\n",
      "Layer: batch_normalization_149, False\n",
      "Layer: activation_149, False\n",
      "Layer: conv2d_150, False\n",
      "Layer: batch_normalization_150, False\n",
      "Layer: activation_150, False\n",
      "Layer: conv2d_148, False\n",
      "Layer: conv2d_151, False\n",
      "Layer: batch_normalization_148, False\n",
      "Layer: batch_normalization_151, False\n",
      "Layer: activation_148, False\n",
      "Layer: activation_151, False\n",
      "Layer: block17_19_mixed, False\n",
      "Layer: block17_19_conv, False\n",
      "Layer: custom_scale_layer_28, False\n",
      "Layer: block17_19_ac, False\n",
      "Layer: conv2d_153, False\n",
      "Layer: batch_normalization_153, False\n",
      "Layer: activation_153, False\n",
      "Layer: conv2d_154, False\n",
      "Layer: batch_normalization_154, False\n",
      "Layer: activation_154, False\n",
      "Layer: conv2d_152, False\n",
      "Layer: conv2d_155, False\n",
      "Layer: batch_normalization_152, False\n",
      "Layer: batch_normalization_155, False\n",
      "Layer: activation_152, False\n",
      "Layer: activation_155, False\n",
      "Layer: block17_20_mixed, False\n",
      "Layer: block17_20_conv, False\n",
      "Layer: custom_scale_layer_29, False\n",
      "Layer: block17_20_ac, False\n",
      "Layer: conv2d_160, False\n",
      "Layer: batch_normalization_160, False\n",
      "Layer: activation_160, False\n",
      "Layer: conv2d_156, False\n",
      "Layer: conv2d_158, False\n",
      "Layer: conv2d_161, False\n",
      "Layer: batch_normalization_156, False\n",
      "Layer: batch_normalization_158, False\n",
      "Layer: batch_normalization_161, False\n",
      "Layer: activation_156, False\n",
      "Layer: activation_158, False\n",
      "Layer: activation_161, False\n",
      "Layer: conv2d_157, False\n",
      "Layer: conv2d_159, False\n",
      "Layer: conv2d_162, False\n",
      "Layer: batch_normalization_157, False\n",
      "Layer: batch_normalization_159, False\n",
      "Layer: batch_normalization_162, False\n",
      "Layer: activation_157, False\n",
      "Layer: activation_159, False\n",
      "Layer: activation_162, False\n",
      "Layer: max_pooling2d_3, False\n",
      "Layer: mixed_7a, False\n",
      "Layer: conv2d_164, False\n",
      "Layer: batch_normalization_164, False\n",
      "Layer: activation_164, False\n",
      "Layer: conv2d_165, False\n",
      "Layer: batch_normalization_165, False\n",
      "Layer: activation_165, False\n",
      "Layer: conv2d_163, False\n",
      "Layer: conv2d_166, False\n",
      "Layer: batch_normalization_163, False\n",
      "Layer: batch_normalization_166, False\n",
      "Layer: activation_163, False\n",
      "Layer: activation_166, False\n",
      "Layer: block8_1_mixed, False\n",
      "Layer: block8_1_conv, False\n",
      "Layer: custom_scale_layer_30, False\n",
      "Layer: block8_1_ac, False\n",
      "Layer: conv2d_168, False\n",
      "Layer: batch_normalization_168, False\n",
      "Layer: activation_168, False\n",
      "Layer: conv2d_169, False\n",
      "Layer: batch_normalization_169, False\n",
      "Layer: activation_169, False\n",
      "Layer: conv2d_167, False\n",
      "Layer: conv2d_170, False\n",
      "Layer: batch_normalization_167, False\n",
      "Layer: batch_normalization_170, False\n",
      "Layer: activation_167, False\n",
      "Layer: activation_170, False\n",
      "Layer: block8_2_mixed, False\n",
      "Layer: block8_2_conv, False\n",
      "Layer: custom_scale_layer_31, False\n",
      "Layer: block8_2_ac, False\n",
      "Layer: conv2d_172, False\n",
      "Layer: batch_normalization_172, False\n",
      "Layer: activation_172, False\n",
      "Layer: conv2d_173, False\n",
      "Layer: batch_normalization_173, False\n",
      "Layer: activation_173, False\n",
      "Layer: conv2d_171, False\n",
      "Layer: conv2d_174, False\n",
      "Layer: batch_normalization_171, False\n",
      "Layer: batch_normalization_174, False\n",
      "Layer: activation_171, False\n",
      "Layer: activation_174, False\n",
      "Layer: block8_3_mixed, False\n",
      "Layer: block8_3_conv, False\n",
      "Layer: custom_scale_layer_32, False\n",
      "Layer: block8_3_ac, False\n",
      "Layer: conv2d_176, False\n",
      "Layer: batch_normalization_176, False\n",
      "Layer: activation_176, False\n",
      "Layer: conv2d_177, False\n",
      "Layer: batch_normalization_177, False\n",
      "Layer: activation_177, False\n",
      "Layer: conv2d_175, False\n",
      "Layer: conv2d_178, False\n",
      "Layer: batch_normalization_175, False\n",
      "Layer: batch_normalization_178, False\n",
      "Layer: activation_175, False\n",
      "Layer: activation_178, False\n",
      "Layer: block8_4_mixed, False\n",
      "Layer: block8_4_conv, False\n",
      "Layer: custom_scale_layer_33, False\n",
      "Layer: block8_4_ac, False\n",
      "Layer: conv2d_180, False\n",
      "Layer: batch_normalization_180, False\n",
      "Layer: activation_180, False\n",
      "Layer: conv2d_181, False\n",
      "Layer: batch_normalization_181, False\n",
      "Layer: activation_181, False\n",
      "Layer: conv2d_179, False\n",
      "Layer: conv2d_182, False\n",
      "Layer: batch_normalization_179, False\n",
      "Layer: batch_normalization_182, False\n",
      "Layer: activation_179, False\n",
      "Layer: activation_182, False\n",
      "Layer: block8_5_mixed, False\n",
      "Layer: block8_5_conv, False\n",
      "Layer: custom_scale_layer_34, False\n",
      "Layer: block8_5_ac, False\n",
      "Layer: conv2d_184, False\n",
      "Layer: batch_normalization_184, False\n",
      "Layer: activation_184, False\n",
      "Layer: conv2d_185, False\n",
      "Layer: batch_normalization_185, False\n",
      "Layer: activation_185, False\n",
      "Layer: conv2d_183, False\n",
      "Layer: conv2d_186, False\n",
      "Layer: batch_normalization_183, False\n",
      "Layer: batch_normalization_186, False\n",
      "Layer: activation_183, False\n",
      "Layer: activation_186, False\n",
      "Layer: block8_6_mixed, False\n",
      "Layer: block8_6_conv, False\n",
      "Layer: custom_scale_layer_35, False\n",
      "Layer: block8_6_ac, False\n",
      "Layer: conv2d_188, False\n",
      "Layer: batch_normalization_188, False\n",
      "Layer: activation_188, False\n",
      "Layer: conv2d_189, False\n",
      "Layer: batch_normalization_189, False\n",
      "Layer: activation_189, False\n",
      "Layer: conv2d_187, False\n",
      "Layer: conv2d_190, False\n",
      "Layer: batch_normalization_187, False\n",
      "Layer: batch_normalization_190, False\n",
      "Layer: activation_187, False\n",
      "Layer: activation_190, False\n",
      "Layer: block8_7_mixed, False\n",
      "Layer: block8_7_conv, False\n",
      "Layer: custom_scale_layer_36, False\n",
      "Layer: block8_7_ac, False\n",
      "Layer: conv2d_192, False\n",
      "Layer: batch_normalization_192, False\n",
      "Layer: activation_192, False\n",
      "Layer: conv2d_193, False\n",
      "Layer: batch_normalization_193, False\n",
      "Layer: activation_193, False\n",
      "Layer: conv2d_191, False\n",
      "Layer: conv2d_194, False\n",
      "Layer: batch_normalization_191, False\n",
      "Layer: batch_normalization_194, False\n",
      "Layer: activation_191, False\n",
      "Layer: activation_194, False\n",
      "Layer: block8_8_mixed, False\n",
      "Layer: block8_8_conv, False\n",
      "Layer: custom_scale_layer_37, False\n",
      "Layer: block8_8_ac, False\n",
      "Layer: conv2d_196, False\n",
      "Layer: batch_normalization_196, False\n",
      "Layer: activation_196, False\n",
      "Layer: conv2d_197, False\n",
      "Layer: batch_normalization_197, False\n",
      "Layer: activation_197, False\n",
      "Layer: conv2d_195, False\n",
      "Layer: conv2d_198, False\n",
      "Layer: batch_normalization_195, False\n",
      "Layer: batch_normalization_198, False\n",
      "Layer: activation_195, False\n",
      "Layer: activation_198, False\n",
      "Layer: block8_9_mixed, False\n",
      "Layer: block8_9_conv, False\n",
      "Layer: custom_scale_layer_38, False\n",
      "Layer: block8_9_ac, False\n",
      "Layer: conv2d_200, False\n",
      "Layer: batch_normalization_200, False\n",
      "Layer: activation_200, False\n",
      "Layer: conv2d_201, False\n",
      "Layer: batch_normalization_201, False\n",
      "Layer: activation_201, False\n",
      "Layer: conv2d_199, False\n",
      "Layer: conv2d_202, False\n",
      "Layer: batch_normalization_199, False\n",
      "Layer: batch_normalization_202, False\n",
      "Layer: activation_199, False\n",
      "Layer: activation_202, False\n",
      "Layer: block8_10_mixed, False\n",
      "Layer: block8_10_conv, False\n",
      "Layer: custom_scale_layer_39, False\n",
      "Layer: conv_7b, False\n",
      "Layer: conv_7b_bn, False\n",
      "Layer: conv_7b_ac, False\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:07.948837Z",
     "start_time": "2024-07-26T01:17:07.945773Z"
    }
   },
   "cell_type": "code",
   "source": "len(base_model.layers)",
   "id": "4d5901824515235e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:08.593815Z",
     "start_time": "2024-07-26T01:17:08.591242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "No_of_classes = len(os.listdir(fundus_train))\n",
    "No_of_classes"
   ],
   "id": "c24aa2a52df99eca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:43.980928Z",
     "start_time": "2024-07-26T01:17:43.261884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAvgPool2D, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3), name = 'Input_layer')\n",
    "x = base_model(inputs, training = False)\n",
    "x = layers.GlobalAvgPool2D()(x)\n",
    "\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "Outputs = Dense(No_of_classes, activation = 'softmax', dtype = tf.float32)(x)\n",
    "\n",
    "model_1 = Model(inputs, Outputs, name = 'InceptionResnetV2')\n",
    "\n",
    "model_1.summary()"
   ],
   "id": "f8314db62f9c88ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"InceptionResnetV2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Funct  (None, 8, 8, 1536)        54336736  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1536)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55551076 (211.91 MB)\n",
      "Trainable params: 1214340 (4.63 MB)\n",
      "Non-trainable params: 54336736 (207.28 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:43.984198Z",
     "start_time": "2024-07-26T01:17:43.982151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "model_1chkpt = ModelCheckpoint(filepath = os.path.join('Trained_Models',model_1.name), save_weights_only = False, save_best_only = True, verbose = 1)"
   ],
   "id": "a30cb8302eb2cd34",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:43.991711Z",
     "start_time": "2024-07-26T01:17:43.984718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.3, patience=2) "
   ],
   "id": "48463ce3a6a20776",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:17:44.163505Z",
     "start_time": "2024-07-26T01:17:44.151026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model compilation\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])"
   ],
   "id": "18264d9b28fa328f",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:21:34.504309Z",
     "start_time": "2024-07-26T01:17:44.742142Z"
    }
   },
   "cell_type": "code",
   "source": "history_1 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = 10, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "85e1fa6dc6835069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:17:49.591493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2024-07-25 21:17:49.657069: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/74 [..............................] - ETA: 8:12 - loss: 1.7512 - accuracy: 0.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 21:17:51.371816: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x63e0aa240a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-25 21:17:51.371849: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-07-25 21:17:51.379531: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-25 21:17:51.443673: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - ETA: 0s - loss: 1.2511 - accuracy: 0.4227\n",
      "Epoch 1: val_loss improved from inf to 1.29353, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 41s 464ms/step - loss: 1.2511 - accuracy: 0.4227 - val_loss: 1.2935 - val_accuracy: 0.0633 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.8764 - accuracy: 0.5947\n",
      "Epoch 2: val_loss improved from 1.29353 to 1.01725, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 31s 420ms/step - loss: 0.8764 - accuracy: 0.5947 - val_loss: 1.0173 - val_accuracy: 0.5806 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7775 - accuracy: 0.6789\n",
      "Epoch 3: val_loss improved from 1.01725 to 0.64071, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 31s 412ms/step - loss: 0.7775 - accuracy: 0.6789 - val_loss: 0.6407 - val_accuracy: 0.8981 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6957 - accuracy: 0.7260\n",
      "Epoch 4: val_loss did not improve from 0.64071\n",
      "74/74 [==============================] - 8s 104ms/step - loss: 0.6957 - accuracy: 0.7260 - val_loss: 0.9050 - val_accuracy: 0.6677 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6620 - accuracy: 0.7378\n",
      "Epoch 5: val_loss improved from 0.64071 to 0.52739, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 32s 426ms/step - loss: 0.6620 - accuracy: 0.7378 - val_loss: 0.5274 - val_accuracy: 0.8635 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6402 - accuracy: 0.7582\n",
      "Epoch 6: val_loss did not improve from 0.52739\n",
      "74/74 [==============================] - 8s 104ms/step - loss: 0.6402 - accuracy: 0.7582 - val_loss: 0.6305 - val_accuracy: 0.8190 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6396 - accuracy: 0.7539\n",
      "Epoch 7: val_loss did not improve from 0.52739\n",
      "74/74 [==============================] - 8s 104ms/step - loss: 0.6396 - accuracy: 0.7539 - val_loss: 0.6015 - val_accuracy: 0.8843 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.7963\n",
      "Epoch 8: val_loss did not improve from 0.52739\n",
      "74/74 [==============================] - 8s 103ms/step - loss: 0.5692 - accuracy: 0.7963 - val_loss: 0.6025 - val_accuracy: 0.8694 - lr: 3.0000e-04\n",
      "Epoch 9/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.5146 - accuracy: 0.8124\n",
      "Epoch 9: val_loss improved from 0.52739 to 0.49849, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 33s 441ms/step - loss: 0.5146 - accuracy: 0.8124 - val_loss: 0.4985 - val_accuracy: 0.8971 - lr: 3.0000e-04\n",
      "Epoch 10/10\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.8081\n",
      "Epoch 10: val_loss improved from 0.49849 to 0.48134, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 31s 421ms/step - loss: 0.5249 - accuracy: 0.8081 - val_loss: 0.4813 - val_accuracy: 0.8902 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:21:49.037769Z",
     "start_time": "2024-07-26T01:21:49.022520Z"
    }
   },
   "cell_type": "code",
   "source": "base_model.trainable = True",
   "id": "9b7e0fe022b2e5ad",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:21:50.255443Z",
     "start_time": "2024-07-26T01:21:50.241237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers[:-50]:\n",
    "    layer.trainable = False"
   ],
   "id": "144c133d132993be",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:21:51.006931Z",
     "start_time": "2024-07-26T01:21:51.001953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers:\n",
    "    print(layer.trainable)"
   ],
   "id": "a853d58f3af9fa71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:22:49.030346Z",
     "start_time": "2024-07-26T01:22:48.977717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 10\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0005), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "7233597bf79998e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"InceptionResnetV2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Funct  (None, 8, 8, 1536)        54336736  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1536)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55551076 (211.91 MB)\n",
      "Trainable params: 10514436 (40.11 MB)\n",
      "Non-trainable params: 45036640 (171.80 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:26:10.227401Z",
     "start_time": "2024-07-26T01:22:51.673727Z"
    }
   },
   "cell_type": "code",
   "source": "history_50 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "eeb24a49a8759ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.4855 - accuracy: 0.5993\n",
      "Epoch 11: val_loss did not improve from 0.48134\n",
      "74/74 [==============================] - 18s 135ms/step - loss: 2.4855 - accuracy: 0.5993 - val_loss: 0.5957 - val_accuracy: 0.8160 - lr: 5.0000e-04\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4873 - accuracy: 0.8204\n",
      "Epoch 12: val_loss improved from 0.48134 to 0.37528, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 34s 466ms/step - loss: 0.4873 - accuracy: 0.8204 - val_loss: 0.3753 - val_accuracy: 0.9486 - lr: 5.0000e-04\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.8691\n",
      "Epoch 13: val_loss did not improve from 0.37528\n",
      "74/74 [==============================] - 9s 113ms/step - loss: 0.3451 - accuracy: 0.8691 - val_loss: 0.5124 - val_accuracy: 0.8249 - lr: 5.0000e-04\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.8848\n",
      "Epoch 14: val_loss did not improve from 0.37528\n",
      "74/74 [==============================] - 9s 113ms/step - loss: 0.3309 - accuracy: 0.8848 - val_loss: 0.4967 - val_accuracy: 0.9080 - lr: 5.0000e-04\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.2823 - accuracy: 0.9064\n",
      "Epoch 15: val_loss improved from 0.37528 to 0.27351, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 35s 468ms/step - loss: 0.2823 - accuracy: 0.9064 - val_loss: 0.2735 - val_accuracy: 0.9426 - lr: 1.5000e-04\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9255\n",
      "Epoch 16: val_loss improved from 0.27351 to 0.26654, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 33s 448ms/step - loss: 0.2103 - accuracy: 0.9255 - val_loss: 0.2665 - val_accuracy: 0.9466 - lr: 1.5000e-04\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1978 - accuracy: 0.9301\n",
      "Epoch 17: val_loss improved from 0.26654 to 0.20662, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 35s 474ms/step - loss: 0.1978 - accuracy: 0.9301 - val_loss: 0.2066 - val_accuracy: 0.9535 - lr: 1.5000e-04\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1771 - accuracy: 0.9441\n",
      "Epoch 18: val_loss did not improve from 0.20662\n",
      "74/74 [==============================] - 9s 112ms/step - loss: 0.1771 - accuracy: 0.9441 - val_loss: 0.2882 - val_accuracy: 0.9228 - lr: 1.5000e-04\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1533 - accuracy: 0.9509\n",
      "Epoch 19: val_loss did not improve from 0.20662\n",
      "74/74 [==============================] - 9s 116ms/step - loss: 0.1533 - accuracy: 0.9509 - val_loss: 0.2625 - val_accuracy: 0.9436 - lr: 1.5000e-04\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9644\n",
      "Epoch 20: val_loss did not improve from 0.20662\n",
      "74/74 [==============================] - 8s 111ms/step - loss: 0.1177 - accuracy: 0.9644 - val_loss: 0.2349 - val_accuracy: 0.9476 - lr: 4.5000e-05\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:26:38.351902Z",
     "start_time": "2024-07-26T01:26:38.347463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 20\n",
    "for layer in base_model.layers[-150:]:\n",
    "    layer.trainable = True"
   ],
   "id": "6a7cd8da92ae1b49",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:26:38.795999Z",
     "start_time": "2024-07-26T01:26:38.744760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0003), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "1e494e7e29768448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"InceptionResnetV2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Funct  (None, 8, 8, 1536)        54336736  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1536)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55551076 (211.91 MB)\n",
      "Trainable params: 23655716 (90.24 MB)\n",
      "Non-trainable params: 31895360 (121.67 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:29:47.525282Z",
     "start_time": "2024-07-26T01:26:39.777540Z"
    }
   },
   "cell_type": "code",
   "source": "history_100 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "81d7f639996a2cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6330 - accuracy: 0.8014\n",
      "Epoch 21: val_loss did not improve from 0.20662\n",
      "74/74 [==============================] - 22s 190ms/step - loss: 0.6330 - accuracy: 0.8014 - val_loss: 0.3054 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 22/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.2950 - accuracy: 0.8971\n",
      "Epoch 22: val_loss improved from 0.20662 to 0.19147, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 36s 492ms/step - loss: 0.2950 - accuracy: 0.8971 - val_loss: 0.1915 - val_accuracy: 0.9387 - lr: 3.0000e-04\n",
      "Epoch 23/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9314\n",
      "Epoch 23: val_loss did not improve from 0.19147\n",
      "74/74 [==============================] - 10s 124ms/step - loss: 0.2024 - accuracy: 0.9314 - val_loss: 0.2295 - val_accuracy: 0.9228 - lr: 3.0000e-04\n",
      "Epoch 24/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1491 - accuracy: 0.9475\n",
      "Epoch 24: val_loss did not improve from 0.19147\n",
      "74/74 [==============================] - 10s 124ms/step - loss: 0.1491 - accuracy: 0.9475 - val_loss: 0.2576 - val_accuracy: 0.9298 - lr: 3.0000e-04\n",
      "Epoch 25/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9716\n",
      "Epoch 25: val_loss improved from 0.19147 to 0.17562, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 36s 481ms/step - loss: 0.0909 - accuracy: 0.9716 - val_loss: 0.1756 - val_accuracy: 0.9585 - lr: 9.0000e-05\n",
      "Epoch 26/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9814\n",
      "Epoch 26: val_loss did not improve from 0.17562\n",
      "74/74 [==============================] - 10s 125ms/step - loss: 0.0638 - accuracy: 0.9814 - val_loss: 0.2213 - val_accuracy: 0.9397 - lr: 9.0000e-05\n",
      "Epoch 27/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0482 - accuracy: 0.9835\n",
      "Epoch 27: val_loss improved from 0.17562 to 0.13797, saving model to Trained_Models/InceptionResnetV2\n",
      "74/74 [==============================] - 37s 495ms/step - loss: 0.0482 - accuracy: 0.9835 - val_loss: 0.1380 - val_accuracy: 0.9634 - lr: 9.0000e-05\n",
      "Epoch 28/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9881\n",
      "Epoch 28: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 9s 121ms/step - loss: 0.0406 - accuracy: 0.9881 - val_loss: 0.2122 - val_accuracy: 0.9575 - lr: 9.0000e-05\n",
      "Epoch 29/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9949\n",
      "Epoch 29: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 9s 123ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.2108 - val_accuracy: 0.9644 - lr: 9.0000e-05\n",
      "Epoch 30/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9975\n",
      "Epoch 30: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 10s 130ms/step - loss: 0.0113 - accuracy: 0.9975 - val_loss: 0.2259 - val_accuracy: 0.9674 - lr: 2.7000e-05\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:29:47.666595Z",
     "start_time": "2024-07-26T01:29:47.659829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 30\n",
    "for layer in base_model.layers[-400:]:\n",
    "    layer.trainable = True"
   ],
   "id": "9c3fa202d236af9e",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:29:55.284707Z",
     "start_time": "2024-07-26T01:29:55.235713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0001), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "10b77e72c3b9b02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"InceptionResnetV2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Funct  (None, 8, 8, 1536)        54336736  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1536)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55551076 (211.91 MB)\n",
      "Trainable params: 43693796 (166.68 MB)\n",
      "Non-trainable params: 11857280 (45.23 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:32:11.186180Z",
     "start_time": "2024-07-26T01:30:05.051171Z"
    }
   },
   "cell_type": "code",
   "source": "history_200 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+8, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "38d94ee2c181b1df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4077 - accuracy: 0.9140\n",
      "Epoch 31: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 32s 195ms/step - loss: 0.4077 - accuracy: 0.9140 - val_loss: 0.3088 - val_accuracy: 0.9268 - lr: 1.0000e-04\n",
      "Epoch 32/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9564\n",
      "Epoch 32: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 13s 170ms/step - loss: 0.1364 - accuracy: 0.9564 - val_loss: 0.1730 - val_accuracy: 0.9693 - lr: 1.0000e-04\n",
      "Epoch 33/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9593\n",
      "Epoch 33: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 13s 171ms/step - loss: 0.1315 - accuracy: 0.9593 - val_loss: 0.1921 - val_accuracy: 0.9594 - lr: 1.0000e-04\n",
      "Epoch 34/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9848\n",
      "Epoch 34: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 13s 171ms/step - loss: 0.0523 - accuracy: 0.9848 - val_loss: 0.1897 - val_accuracy: 0.9713 - lr: 1.0000e-04\n",
      "Epoch 35/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9945\n",
      "Epoch 35: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 13s 172ms/step - loss: 0.0181 - accuracy: 0.9945 - val_loss: 0.2345 - val_accuracy: 0.9644 - lr: 3.0000e-05\n",
      "Epoch 36/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0074 - accuracy: 0.9987\n",
      "Epoch 36: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 13s 172ms/step - loss: 0.0074 - accuracy: 0.9987 - val_loss: 0.2554 - val_accuracy: 0.9634 - lr: 3.0000e-05\n",
      "Epoch 37/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 37: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 15s 194ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2504 - val_accuracy: 0.9664 - lr: 9.0000e-06\n",
      "Epoch 38/38\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 38: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 14s 184ms/step - loss: 0.0045 - accuracy: 0.9992 - val_loss: 0.2630 - val_accuracy: 0.9644 - lr: 9.0000e-06\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:32:35.523802Z",
     "start_time": "2024-07-26T01:32:35.512861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 38\n",
    "for layer in base_model.layers[-770:]:\n",
    "    layer.trainable = True"
   ],
   "id": "f24912d6c3aa128",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:32:36.083921Z",
     "start_time": "2024-07-26T01:32:36.024198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0000015), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "81ac6230c263bbc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"InceptionResnetV2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 299, 299, 3)]     0         \n",
      "                                                                 \n",
      " inception_resnet_v2 (Funct  (None, 8, 8, 1536)        54336736  \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 1536)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               786944    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 516       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55551076 (211.91 MB)\n",
      "Trainable params: 55461892 (211.57 MB)\n",
      "Non-trainable params: 89184 (348.38 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:33:54.140416Z",
     "start_time": "2024-07-26T01:32:42.181237Z"
    }
   },
   "cell_type": "code",
   "source": "history_250 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+5, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler])",
   "id": "fd98979b64e68f4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/43\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9987\n",
      "Epoch 39: val_loss did not improve from 0.13797\n",
      "74/74 [==============================] - 68s 277ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.2814 - val_accuracy: 0.9654 - lr: 1.5000e-06\n",
      "Epoch 40/43\n",
      "16/74 [=====>........................] - ETA: 12s - loss: 0.0016 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m history_250 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_datagen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_datagen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodel_1chkpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1776\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1777\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1780\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1781\u001B[0m ):\n\u001B[1;32m   1782\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1783\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1785\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    828\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 831\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    833\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    834\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    864\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    865\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    866\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 867\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    872\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    873\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1260\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1262\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1263\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1264\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1265\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1266\u001B[0m     args,\n\u001B[1;32m   1267\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1268\u001B[0m     executing_eagerly)\n\u001B[1;32m   1269\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001B[0m, in \u001B[0;36mAtomicFunction.flat_call\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflat_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    216\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 217\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001B[0m, in \u001B[0;36mAtomicFunction.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    251\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 252\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    258\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    261\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    262\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1477\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1479\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1480\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1481\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1482\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1484\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1485\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1486\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1487\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1488\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1489\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1493\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1494\u001B[0m   )\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     53\u001B[0m   \u001B[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001B[39;00m\n\u001B[1;32m     54\u001B[0m   inputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     55\u001B[0m       tensor_conversion_registry\u001B[38;5;241m.\u001B[39mconvert(t)\n\u001B[1;32m     56\u001B[0m       \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, core_types\u001B[38;5;241m.\u001B[39mTensor)\n\u001B[1;32m     57\u001B[0m       \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m     58\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m inputs\n\u001B[1;32m     59\u001B[0m   ]\n\u001B[0;32m---> 60\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     63\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:34:11.239706Z",
     "start_time": "2024-07-26T01:33:57.509287Z"
    }
   },
   "cell_type": "code",
   "source": "MobileNet_Best = tf.keras.models.load_model(\"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/ML_Models/Trained_Models/InceptionResnetV2\")\n",
   "id": "cc8e6b2a803e9d63",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T01:34:15.593076Z",
     "start_time": "2024-07-26T01:34:11.240607Z"
    }
   },
   "cell_type": "code",
   "source": "MobileNet_Best.evaluate(test_datagen)",
   "id": "ebb33e0640d23929",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 106ms/step - loss: 0.4096 - accuracy: 0.9065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.40956565737724304, 0.9065088629722595]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
