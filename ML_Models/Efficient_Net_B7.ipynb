{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:31.126768Z",
     "start_time": "2024-07-27T02:42:27.486449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import utils\n",
    "from tensorflow.keras import mixed_precision\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:42:27.841865: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-26 22:42:28.069538: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-26 22:42:28.069560: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-26 22:42:28.070818: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-26 22:42:28.191216: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 22:42:29.178488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:42:30.993760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.109011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.119205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.121905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:31.132126Z",
     "start_time": "2024-07-27T02:42:31.127711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\"\"\"\n",
    "#Batching using prefetch\n",
    "train_data_casted = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "test_data_casted = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\"\"\""
   ],
   "id": "2bfc02fd3bd68fe0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Batching using prefetch\\ntrain_data_casted = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE)\\ntest_data_casted = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:31.137856Z",
     "start_time": "2024-07-27T02:42:31.132745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fundus_train = \"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/Dataset/split1/train\"\n",
    "fundus_test = \"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/Dataset/split1/test\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n"
   ],
   "id": "76e3a5421f09f74d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:31.141325Z",
     "start_time": "2024-07-27T02:42:31.139054Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.listdir(fundus_train))",
   "id": "1a2371451891c6ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glaucoma', 'normal', 'cataract', 'diabetic_retinopathy']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:31.147242Z",
     "start_time": "2024-07-27T02:42:31.142039Z"
    }
   },
   "cell_type": "code",
   "source": "IMG_HEIGHT, IMG_WIDTH = 224, 224",
   "id": "1f94cba87987258",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:32.274363Z",
     "start_time": "2024-07-27T02:42:31.148027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_train,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.35,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_train,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "    validation_split=0.35,\n",
    "    subset='validation'\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_test,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "\n",
    ")"
   ],
   "id": "348e18c639023b17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3372 files belonging to 4 classes.\n",
      "Using 2192 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:42:31.212655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.215678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.218535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.377448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.379365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.381142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:42:31.383131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5886 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "2024-07-26 22:42:31.622287: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3372 files belonging to 4 classes.\n",
      "Using 1180 files for validation.\n",
      "Found 845 files belonging to 4 classes.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:32.277990Z",
     "start_time": "2024-07-27T02:42:32.275552Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset",
   "id": "d09dab585b1ca859",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:32.282963Z",
     "start_time": "2024-07-27T02:42:32.278705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Using tf prefetch dataset\n",
    "preprocess_input = tf.keras.applications.efficientnet.preprocess_input"
   ],
   "id": "c66d836742f62b76",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:42:32.315007Z",
     "start_time": "2024-07-27T02:42:32.283774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datagen = train_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "val_datagen = validation_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "test_datagen = test_dataset.map(lambda x, y: (preprocess_input(x), y))"
   ],
   "id": "fe651f3421b86187",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:33:44.620162Z",
     "start_time": "2024-07-27T00:33:44.617517Z"
    }
   },
   "cell_type": "code",
   "source": "train_datagen",
   "id": "2d604d0924419863",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:33:49.239994Z",
     "start_time": "2024-07-27T00:33:45.661153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow.keras.applications as apps\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "base_model = apps.EfficientNetB7(weights = 'imagenet', include_top = False, input_shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.trainable = False"
   ],
   "id": "541ea676b52829f7",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:33:49.247280Z",
     "start_time": "2024-07-27T00:33:49.241097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for i in base_model.layers:\n",
    "    print(f'Layer: {i.name}, {i.trainable}')"
   ],
   "id": "79db08db0282846b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_1, False\n",
      "Layer: rescaling, False\n",
      "Layer: normalization, False\n",
      "Layer: rescaling_1, False\n",
      "Layer: stem_conv_pad, False\n",
      "Layer: stem_conv, False\n",
      "Layer: stem_bn, False\n",
      "Layer: stem_activation, False\n",
      "Layer: block1a_dwconv, False\n",
      "Layer: block1a_bn, False\n",
      "Layer: block1a_activation, False\n",
      "Layer: block1a_se_squeeze, False\n",
      "Layer: block1a_se_reshape, False\n",
      "Layer: block1a_se_reduce, False\n",
      "Layer: block1a_se_expand, False\n",
      "Layer: block1a_se_excite, False\n",
      "Layer: block1a_project_conv, False\n",
      "Layer: block1a_project_bn, False\n",
      "Layer: block1b_dwconv, False\n",
      "Layer: block1b_bn, False\n",
      "Layer: block1b_activation, False\n",
      "Layer: block1b_se_squeeze, False\n",
      "Layer: block1b_se_reshape, False\n",
      "Layer: block1b_se_reduce, False\n",
      "Layer: block1b_se_expand, False\n",
      "Layer: block1b_se_excite, False\n",
      "Layer: block1b_project_conv, False\n",
      "Layer: block1b_project_bn, False\n",
      "Layer: block1b_drop, False\n",
      "Layer: block1b_add, False\n",
      "Layer: block1c_dwconv, False\n",
      "Layer: block1c_bn, False\n",
      "Layer: block1c_activation, False\n",
      "Layer: block1c_se_squeeze, False\n",
      "Layer: block1c_se_reshape, False\n",
      "Layer: block1c_se_reduce, False\n",
      "Layer: block1c_se_expand, False\n",
      "Layer: block1c_se_excite, False\n",
      "Layer: block1c_project_conv, False\n",
      "Layer: block1c_project_bn, False\n",
      "Layer: block1c_drop, False\n",
      "Layer: block1c_add, False\n",
      "Layer: block1d_dwconv, False\n",
      "Layer: block1d_bn, False\n",
      "Layer: block1d_activation, False\n",
      "Layer: block1d_se_squeeze, False\n",
      "Layer: block1d_se_reshape, False\n",
      "Layer: block1d_se_reduce, False\n",
      "Layer: block1d_se_expand, False\n",
      "Layer: block1d_se_excite, False\n",
      "Layer: block1d_project_conv, False\n",
      "Layer: block1d_project_bn, False\n",
      "Layer: block1d_drop, False\n",
      "Layer: block1d_add, False\n",
      "Layer: block2a_expand_conv, False\n",
      "Layer: block2a_expand_bn, False\n",
      "Layer: block2a_expand_activation, False\n",
      "Layer: block2a_dwconv_pad, False\n",
      "Layer: block2a_dwconv, False\n",
      "Layer: block2a_bn, False\n",
      "Layer: block2a_activation, False\n",
      "Layer: block2a_se_squeeze, False\n",
      "Layer: block2a_se_reshape, False\n",
      "Layer: block2a_se_reduce, False\n",
      "Layer: block2a_se_expand, False\n",
      "Layer: block2a_se_excite, False\n",
      "Layer: block2a_project_conv, False\n",
      "Layer: block2a_project_bn, False\n",
      "Layer: block2b_expand_conv, False\n",
      "Layer: block2b_expand_bn, False\n",
      "Layer: block2b_expand_activation, False\n",
      "Layer: block2b_dwconv, False\n",
      "Layer: block2b_bn, False\n",
      "Layer: block2b_activation, False\n",
      "Layer: block2b_se_squeeze, False\n",
      "Layer: block2b_se_reshape, False\n",
      "Layer: block2b_se_reduce, False\n",
      "Layer: block2b_se_expand, False\n",
      "Layer: block2b_se_excite, False\n",
      "Layer: block2b_project_conv, False\n",
      "Layer: block2b_project_bn, False\n",
      "Layer: block2b_drop, False\n",
      "Layer: block2b_add, False\n",
      "Layer: block2c_expand_conv, False\n",
      "Layer: block2c_expand_bn, False\n",
      "Layer: block2c_expand_activation, False\n",
      "Layer: block2c_dwconv, False\n",
      "Layer: block2c_bn, False\n",
      "Layer: block2c_activation, False\n",
      "Layer: block2c_se_squeeze, False\n",
      "Layer: block2c_se_reshape, False\n",
      "Layer: block2c_se_reduce, False\n",
      "Layer: block2c_se_expand, False\n",
      "Layer: block2c_se_excite, False\n",
      "Layer: block2c_project_conv, False\n",
      "Layer: block2c_project_bn, False\n",
      "Layer: block2c_drop, False\n",
      "Layer: block2c_add, False\n",
      "Layer: block2d_expand_conv, False\n",
      "Layer: block2d_expand_bn, False\n",
      "Layer: block2d_expand_activation, False\n",
      "Layer: block2d_dwconv, False\n",
      "Layer: block2d_bn, False\n",
      "Layer: block2d_activation, False\n",
      "Layer: block2d_se_squeeze, False\n",
      "Layer: block2d_se_reshape, False\n",
      "Layer: block2d_se_reduce, False\n",
      "Layer: block2d_se_expand, False\n",
      "Layer: block2d_se_excite, False\n",
      "Layer: block2d_project_conv, False\n",
      "Layer: block2d_project_bn, False\n",
      "Layer: block2d_drop, False\n",
      "Layer: block2d_add, False\n",
      "Layer: block2e_expand_conv, False\n",
      "Layer: block2e_expand_bn, False\n",
      "Layer: block2e_expand_activation, False\n",
      "Layer: block2e_dwconv, False\n",
      "Layer: block2e_bn, False\n",
      "Layer: block2e_activation, False\n",
      "Layer: block2e_se_squeeze, False\n",
      "Layer: block2e_se_reshape, False\n",
      "Layer: block2e_se_reduce, False\n",
      "Layer: block2e_se_expand, False\n",
      "Layer: block2e_se_excite, False\n",
      "Layer: block2e_project_conv, False\n",
      "Layer: block2e_project_bn, False\n",
      "Layer: block2e_drop, False\n",
      "Layer: block2e_add, False\n",
      "Layer: block2f_expand_conv, False\n",
      "Layer: block2f_expand_bn, False\n",
      "Layer: block2f_expand_activation, False\n",
      "Layer: block2f_dwconv, False\n",
      "Layer: block2f_bn, False\n",
      "Layer: block2f_activation, False\n",
      "Layer: block2f_se_squeeze, False\n",
      "Layer: block2f_se_reshape, False\n",
      "Layer: block2f_se_reduce, False\n",
      "Layer: block2f_se_expand, False\n",
      "Layer: block2f_se_excite, False\n",
      "Layer: block2f_project_conv, False\n",
      "Layer: block2f_project_bn, False\n",
      "Layer: block2f_drop, False\n",
      "Layer: block2f_add, False\n",
      "Layer: block2g_expand_conv, False\n",
      "Layer: block2g_expand_bn, False\n",
      "Layer: block2g_expand_activation, False\n",
      "Layer: block2g_dwconv, False\n",
      "Layer: block2g_bn, False\n",
      "Layer: block2g_activation, False\n",
      "Layer: block2g_se_squeeze, False\n",
      "Layer: block2g_se_reshape, False\n",
      "Layer: block2g_se_reduce, False\n",
      "Layer: block2g_se_expand, False\n",
      "Layer: block2g_se_excite, False\n",
      "Layer: block2g_project_conv, False\n",
      "Layer: block2g_project_bn, False\n",
      "Layer: block2g_drop, False\n",
      "Layer: block2g_add, False\n",
      "Layer: block3a_expand_conv, False\n",
      "Layer: block3a_expand_bn, False\n",
      "Layer: block3a_expand_activation, False\n",
      "Layer: block3a_dwconv_pad, False\n",
      "Layer: block3a_dwconv, False\n",
      "Layer: block3a_bn, False\n",
      "Layer: block3a_activation, False\n",
      "Layer: block3a_se_squeeze, False\n",
      "Layer: block3a_se_reshape, False\n",
      "Layer: block3a_se_reduce, False\n",
      "Layer: block3a_se_expand, False\n",
      "Layer: block3a_se_excite, False\n",
      "Layer: block3a_project_conv, False\n",
      "Layer: block3a_project_bn, False\n",
      "Layer: block3b_expand_conv, False\n",
      "Layer: block3b_expand_bn, False\n",
      "Layer: block3b_expand_activation, False\n",
      "Layer: block3b_dwconv, False\n",
      "Layer: block3b_bn, False\n",
      "Layer: block3b_activation, False\n",
      "Layer: block3b_se_squeeze, False\n",
      "Layer: block3b_se_reshape, False\n",
      "Layer: block3b_se_reduce, False\n",
      "Layer: block3b_se_expand, False\n",
      "Layer: block3b_se_excite, False\n",
      "Layer: block3b_project_conv, False\n",
      "Layer: block3b_project_bn, False\n",
      "Layer: block3b_drop, False\n",
      "Layer: block3b_add, False\n",
      "Layer: block3c_expand_conv, False\n",
      "Layer: block3c_expand_bn, False\n",
      "Layer: block3c_expand_activation, False\n",
      "Layer: block3c_dwconv, False\n",
      "Layer: block3c_bn, False\n",
      "Layer: block3c_activation, False\n",
      "Layer: block3c_se_squeeze, False\n",
      "Layer: block3c_se_reshape, False\n",
      "Layer: block3c_se_reduce, False\n",
      "Layer: block3c_se_expand, False\n",
      "Layer: block3c_se_excite, False\n",
      "Layer: block3c_project_conv, False\n",
      "Layer: block3c_project_bn, False\n",
      "Layer: block3c_drop, False\n",
      "Layer: block3c_add, False\n",
      "Layer: block3d_expand_conv, False\n",
      "Layer: block3d_expand_bn, False\n",
      "Layer: block3d_expand_activation, False\n",
      "Layer: block3d_dwconv, False\n",
      "Layer: block3d_bn, False\n",
      "Layer: block3d_activation, False\n",
      "Layer: block3d_se_squeeze, False\n",
      "Layer: block3d_se_reshape, False\n",
      "Layer: block3d_se_reduce, False\n",
      "Layer: block3d_se_expand, False\n",
      "Layer: block3d_se_excite, False\n",
      "Layer: block3d_project_conv, False\n",
      "Layer: block3d_project_bn, False\n",
      "Layer: block3d_drop, False\n",
      "Layer: block3d_add, False\n",
      "Layer: block3e_expand_conv, False\n",
      "Layer: block3e_expand_bn, False\n",
      "Layer: block3e_expand_activation, False\n",
      "Layer: block3e_dwconv, False\n",
      "Layer: block3e_bn, False\n",
      "Layer: block3e_activation, False\n",
      "Layer: block3e_se_squeeze, False\n",
      "Layer: block3e_se_reshape, False\n",
      "Layer: block3e_se_reduce, False\n",
      "Layer: block3e_se_expand, False\n",
      "Layer: block3e_se_excite, False\n",
      "Layer: block3e_project_conv, False\n",
      "Layer: block3e_project_bn, False\n",
      "Layer: block3e_drop, False\n",
      "Layer: block3e_add, False\n",
      "Layer: block3f_expand_conv, False\n",
      "Layer: block3f_expand_bn, False\n",
      "Layer: block3f_expand_activation, False\n",
      "Layer: block3f_dwconv, False\n",
      "Layer: block3f_bn, False\n",
      "Layer: block3f_activation, False\n",
      "Layer: block3f_se_squeeze, False\n",
      "Layer: block3f_se_reshape, False\n",
      "Layer: block3f_se_reduce, False\n",
      "Layer: block3f_se_expand, False\n",
      "Layer: block3f_se_excite, False\n",
      "Layer: block3f_project_conv, False\n",
      "Layer: block3f_project_bn, False\n",
      "Layer: block3f_drop, False\n",
      "Layer: block3f_add, False\n",
      "Layer: block3g_expand_conv, False\n",
      "Layer: block3g_expand_bn, False\n",
      "Layer: block3g_expand_activation, False\n",
      "Layer: block3g_dwconv, False\n",
      "Layer: block3g_bn, False\n",
      "Layer: block3g_activation, False\n",
      "Layer: block3g_se_squeeze, False\n",
      "Layer: block3g_se_reshape, False\n",
      "Layer: block3g_se_reduce, False\n",
      "Layer: block3g_se_expand, False\n",
      "Layer: block3g_se_excite, False\n",
      "Layer: block3g_project_conv, False\n",
      "Layer: block3g_project_bn, False\n",
      "Layer: block3g_drop, False\n",
      "Layer: block3g_add, False\n",
      "Layer: block4a_expand_conv, False\n",
      "Layer: block4a_expand_bn, False\n",
      "Layer: block4a_expand_activation, False\n",
      "Layer: block4a_dwconv_pad, False\n",
      "Layer: block4a_dwconv, False\n",
      "Layer: block4a_bn, False\n",
      "Layer: block4a_activation, False\n",
      "Layer: block4a_se_squeeze, False\n",
      "Layer: block4a_se_reshape, False\n",
      "Layer: block4a_se_reduce, False\n",
      "Layer: block4a_se_expand, False\n",
      "Layer: block4a_se_excite, False\n",
      "Layer: block4a_project_conv, False\n",
      "Layer: block4a_project_bn, False\n",
      "Layer: block4b_expand_conv, False\n",
      "Layer: block4b_expand_bn, False\n",
      "Layer: block4b_expand_activation, False\n",
      "Layer: block4b_dwconv, False\n",
      "Layer: block4b_bn, False\n",
      "Layer: block4b_activation, False\n",
      "Layer: block4b_se_squeeze, False\n",
      "Layer: block4b_se_reshape, False\n",
      "Layer: block4b_se_reduce, False\n",
      "Layer: block4b_se_expand, False\n",
      "Layer: block4b_se_excite, False\n",
      "Layer: block4b_project_conv, False\n",
      "Layer: block4b_project_bn, False\n",
      "Layer: block4b_drop, False\n",
      "Layer: block4b_add, False\n",
      "Layer: block4c_expand_conv, False\n",
      "Layer: block4c_expand_bn, False\n",
      "Layer: block4c_expand_activation, False\n",
      "Layer: block4c_dwconv, False\n",
      "Layer: block4c_bn, False\n",
      "Layer: block4c_activation, False\n",
      "Layer: block4c_se_squeeze, False\n",
      "Layer: block4c_se_reshape, False\n",
      "Layer: block4c_se_reduce, False\n",
      "Layer: block4c_se_expand, False\n",
      "Layer: block4c_se_excite, False\n",
      "Layer: block4c_project_conv, False\n",
      "Layer: block4c_project_bn, False\n",
      "Layer: block4c_drop, False\n",
      "Layer: block4c_add, False\n",
      "Layer: block4d_expand_conv, False\n",
      "Layer: block4d_expand_bn, False\n",
      "Layer: block4d_expand_activation, False\n",
      "Layer: block4d_dwconv, False\n",
      "Layer: block4d_bn, False\n",
      "Layer: block4d_activation, False\n",
      "Layer: block4d_se_squeeze, False\n",
      "Layer: block4d_se_reshape, False\n",
      "Layer: block4d_se_reduce, False\n",
      "Layer: block4d_se_expand, False\n",
      "Layer: block4d_se_excite, False\n",
      "Layer: block4d_project_conv, False\n",
      "Layer: block4d_project_bn, False\n",
      "Layer: block4d_drop, False\n",
      "Layer: block4d_add, False\n",
      "Layer: block4e_expand_conv, False\n",
      "Layer: block4e_expand_bn, False\n",
      "Layer: block4e_expand_activation, False\n",
      "Layer: block4e_dwconv, False\n",
      "Layer: block4e_bn, False\n",
      "Layer: block4e_activation, False\n",
      "Layer: block4e_se_squeeze, False\n",
      "Layer: block4e_se_reshape, False\n",
      "Layer: block4e_se_reduce, False\n",
      "Layer: block4e_se_expand, False\n",
      "Layer: block4e_se_excite, False\n",
      "Layer: block4e_project_conv, False\n",
      "Layer: block4e_project_bn, False\n",
      "Layer: block4e_drop, False\n",
      "Layer: block4e_add, False\n",
      "Layer: block4f_expand_conv, False\n",
      "Layer: block4f_expand_bn, False\n",
      "Layer: block4f_expand_activation, False\n",
      "Layer: block4f_dwconv, False\n",
      "Layer: block4f_bn, False\n",
      "Layer: block4f_activation, False\n",
      "Layer: block4f_se_squeeze, False\n",
      "Layer: block4f_se_reshape, False\n",
      "Layer: block4f_se_reduce, False\n",
      "Layer: block4f_se_expand, False\n",
      "Layer: block4f_se_excite, False\n",
      "Layer: block4f_project_conv, False\n",
      "Layer: block4f_project_bn, False\n",
      "Layer: block4f_drop, False\n",
      "Layer: block4f_add, False\n",
      "Layer: block4g_expand_conv, False\n",
      "Layer: block4g_expand_bn, False\n",
      "Layer: block4g_expand_activation, False\n",
      "Layer: block4g_dwconv, False\n",
      "Layer: block4g_bn, False\n",
      "Layer: block4g_activation, False\n",
      "Layer: block4g_se_squeeze, False\n",
      "Layer: block4g_se_reshape, False\n",
      "Layer: block4g_se_reduce, False\n",
      "Layer: block4g_se_expand, False\n",
      "Layer: block4g_se_excite, False\n",
      "Layer: block4g_project_conv, False\n",
      "Layer: block4g_project_bn, False\n",
      "Layer: block4g_drop, False\n",
      "Layer: block4g_add, False\n",
      "Layer: block4h_expand_conv, False\n",
      "Layer: block4h_expand_bn, False\n",
      "Layer: block4h_expand_activation, False\n",
      "Layer: block4h_dwconv, False\n",
      "Layer: block4h_bn, False\n",
      "Layer: block4h_activation, False\n",
      "Layer: block4h_se_squeeze, False\n",
      "Layer: block4h_se_reshape, False\n",
      "Layer: block4h_se_reduce, False\n",
      "Layer: block4h_se_expand, False\n",
      "Layer: block4h_se_excite, False\n",
      "Layer: block4h_project_conv, False\n",
      "Layer: block4h_project_bn, False\n",
      "Layer: block4h_drop, False\n",
      "Layer: block4h_add, False\n",
      "Layer: block4i_expand_conv, False\n",
      "Layer: block4i_expand_bn, False\n",
      "Layer: block4i_expand_activation, False\n",
      "Layer: block4i_dwconv, False\n",
      "Layer: block4i_bn, False\n",
      "Layer: block4i_activation, False\n",
      "Layer: block4i_se_squeeze, False\n",
      "Layer: block4i_se_reshape, False\n",
      "Layer: block4i_se_reduce, False\n",
      "Layer: block4i_se_expand, False\n",
      "Layer: block4i_se_excite, False\n",
      "Layer: block4i_project_conv, False\n",
      "Layer: block4i_project_bn, False\n",
      "Layer: block4i_drop, False\n",
      "Layer: block4i_add, False\n",
      "Layer: block4j_expand_conv, False\n",
      "Layer: block4j_expand_bn, False\n",
      "Layer: block4j_expand_activation, False\n",
      "Layer: block4j_dwconv, False\n",
      "Layer: block4j_bn, False\n",
      "Layer: block4j_activation, False\n",
      "Layer: block4j_se_squeeze, False\n",
      "Layer: block4j_se_reshape, False\n",
      "Layer: block4j_se_reduce, False\n",
      "Layer: block4j_se_expand, False\n",
      "Layer: block4j_se_excite, False\n",
      "Layer: block4j_project_conv, False\n",
      "Layer: block4j_project_bn, False\n",
      "Layer: block4j_drop, False\n",
      "Layer: block4j_add, False\n",
      "Layer: block5a_expand_conv, False\n",
      "Layer: block5a_expand_bn, False\n",
      "Layer: block5a_expand_activation, False\n",
      "Layer: block5a_dwconv, False\n",
      "Layer: block5a_bn, False\n",
      "Layer: block5a_activation, False\n",
      "Layer: block5a_se_squeeze, False\n",
      "Layer: block5a_se_reshape, False\n",
      "Layer: block5a_se_reduce, False\n",
      "Layer: block5a_se_expand, False\n",
      "Layer: block5a_se_excite, False\n",
      "Layer: block5a_project_conv, False\n",
      "Layer: block5a_project_bn, False\n",
      "Layer: block5b_expand_conv, False\n",
      "Layer: block5b_expand_bn, False\n",
      "Layer: block5b_expand_activation, False\n",
      "Layer: block5b_dwconv, False\n",
      "Layer: block5b_bn, False\n",
      "Layer: block5b_activation, False\n",
      "Layer: block5b_se_squeeze, False\n",
      "Layer: block5b_se_reshape, False\n",
      "Layer: block5b_se_reduce, False\n",
      "Layer: block5b_se_expand, False\n",
      "Layer: block5b_se_excite, False\n",
      "Layer: block5b_project_conv, False\n",
      "Layer: block5b_project_bn, False\n",
      "Layer: block5b_drop, False\n",
      "Layer: block5b_add, False\n",
      "Layer: block5c_expand_conv, False\n",
      "Layer: block5c_expand_bn, False\n",
      "Layer: block5c_expand_activation, False\n",
      "Layer: block5c_dwconv, False\n",
      "Layer: block5c_bn, False\n",
      "Layer: block5c_activation, False\n",
      "Layer: block5c_se_squeeze, False\n",
      "Layer: block5c_se_reshape, False\n",
      "Layer: block5c_se_reduce, False\n",
      "Layer: block5c_se_expand, False\n",
      "Layer: block5c_se_excite, False\n",
      "Layer: block5c_project_conv, False\n",
      "Layer: block5c_project_bn, False\n",
      "Layer: block5c_drop, False\n",
      "Layer: block5c_add, False\n",
      "Layer: block5d_expand_conv, False\n",
      "Layer: block5d_expand_bn, False\n",
      "Layer: block5d_expand_activation, False\n",
      "Layer: block5d_dwconv, False\n",
      "Layer: block5d_bn, False\n",
      "Layer: block5d_activation, False\n",
      "Layer: block5d_se_squeeze, False\n",
      "Layer: block5d_se_reshape, False\n",
      "Layer: block5d_se_reduce, False\n",
      "Layer: block5d_se_expand, False\n",
      "Layer: block5d_se_excite, False\n",
      "Layer: block5d_project_conv, False\n",
      "Layer: block5d_project_bn, False\n",
      "Layer: block5d_drop, False\n",
      "Layer: block5d_add, False\n",
      "Layer: block5e_expand_conv, False\n",
      "Layer: block5e_expand_bn, False\n",
      "Layer: block5e_expand_activation, False\n",
      "Layer: block5e_dwconv, False\n",
      "Layer: block5e_bn, False\n",
      "Layer: block5e_activation, False\n",
      "Layer: block5e_se_squeeze, False\n",
      "Layer: block5e_se_reshape, False\n",
      "Layer: block5e_se_reduce, False\n",
      "Layer: block5e_se_expand, False\n",
      "Layer: block5e_se_excite, False\n",
      "Layer: block5e_project_conv, False\n",
      "Layer: block5e_project_bn, False\n",
      "Layer: block5e_drop, False\n",
      "Layer: block5e_add, False\n",
      "Layer: block5f_expand_conv, False\n",
      "Layer: block5f_expand_bn, False\n",
      "Layer: block5f_expand_activation, False\n",
      "Layer: block5f_dwconv, False\n",
      "Layer: block5f_bn, False\n",
      "Layer: block5f_activation, False\n",
      "Layer: block5f_se_squeeze, False\n",
      "Layer: block5f_se_reshape, False\n",
      "Layer: block5f_se_reduce, False\n",
      "Layer: block5f_se_expand, False\n",
      "Layer: block5f_se_excite, False\n",
      "Layer: block5f_project_conv, False\n",
      "Layer: block5f_project_bn, False\n",
      "Layer: block5f_drop, False\n",
      "Layer: block5f_add, False\n",
      "Layer: block5g_expand_conv, False\n",
      "Layer: block5g_expand_bn, False\n",
      "Layer: block5g_expand_activation, False\n",
      "Layer: block5g_dwconv, False\n",
      "Layer: block5g_bn, False\n",
      "Layer: block5g_activation, False\n",
      "Layer: block5g_se_squeeze, False\n",
      "Layer: block5g_se_reshape, False\n",
      "Layer: block5g_se_reduce, False\n",
      "Layer: block5g_se_expand, False\n",
      "Layer: block5g_se_excite, False\n",
      "Layer: block5g_project_conv, False\n",
      "Layer: block5g_project_bn, False\n",
      "Layer: block5g_drop, False\n",
      "Layer: block5g_add, False\n",
      "Layer: block5h_expand_conv, False\n",
      "Layer: block5h_expand_bn, False\n",
      "Layer: block5h_expand_activation, False\n",
      "Layer: block5h_dwconv, False\n",
      "Layer: block5h_bn, False\n",
      "Layer: block5h_activation, False\n",
      "Layer: block5h_se_squeeze, False\n",
      "Layer: block5h_se_reshape, False\n",
      "Layer: block5h_se_reduce, False\n",
      "Layer: block5h_se_expand, False\n",
      "Layer: block5h_se_excite, False\n",
      "Layer: block5h_project_conv, False\n",
      "Layer: block5h_project_bn, False\n",
      "Layer: block5h_drop, False\n",
      "Layer: block5h_add, False\n",
      "Layer: block5i_expand_conv, False\n",
      "Layer: block5i_expand_bn, False\n",
      "Layer: block5i_expand_activation, False\n",
      "Layer: block5i_dwconv, False\n",
      "Layer: block5i_bn, False\n",
      "Layer: block5i_activation, False\n",
      "Layer: block5i_se_squeeze, False\n",
      "Layer: block5i_se_reshape, False\n",
      "Layer: block5i_se_reduce, False\n",
      "Layer: block5i_se_expand, False\n",
      "Layer: block5i_se_excite, False\n",
      "Layer: block5i_project_conv, False\n",
      "Layer: block5i_project_bn, False\n",
      "Layer: block5i_drop, False\n",
      "Layer: block5i_add, False\n",
      "Layer: block5j_expand_conv, False\n",
      "Layer: block5j_expand_bn, False\n",
      "Layer: block5j_expand_activation, False\n",
      "Layer: block5j_dwconv, False\n",
      "Layer: block5j_bn, False\n",
      "Layer: block5j_activation, False\n",
      "Layer: block5j_se_squeeze, False\n",
      "Layer: block5j_se_reshape, False\n",
      "Layer: block5j_se_reduce, False\n",
      "Layer: block5j_se_expand, False\n",
      "Layer: block5j_se_excite, False\n",
      "Layer: block5j_project_conv, False\n",
      "Layer: block5j_project_bn, False\n",
      "Layer: block5j_drop, False\n",
      "Layer: block5j_add, False\n",
      "Layer: block6a_expand_conv, False\n",
      "Layer: block6a_expand_bn, False\n",
      "Layer: block6a_expand_activation, False\n",
      "Layer: block6a_dwconv_pad, False\n",
      "Layer: block6a_dwconv, False\n",
      "Layer: block6a_bn, False\n",
      "Layer: block6a_activation, False\n",
      "Layer: block6a_se_squeeze, False\n",
      "Layer: block6a_se_reshape, False\n",
      "Layer: block6a_se_reduce, False\n",
      "Layer: block6a_se_expand, False\n",
      "Layer: block6a_se_excite, False\n",
      "Layer: block6a_project_conv, False\n",
      "Layer: block6a_project_bn, False\n",
      "Layer: block6b_expand_conv, False\n",
      "Layer: block6b_expand_bn, False\n",
      "Layer: block6b_expand_activation, False\n",
      "Layer: block6b_dwconv, False\n",
      "Layer: block6b_bn, False\n",
      "Layer: block6b_activation, False\n",
      "Layer: block6b_se_squeeze, False\n",
      "Layer: block6b_se_reshape, False\n",
      "Layer: block6b_se_reduce, False\n",
      "Layer: block6b_se_expand, False\n",
      "Layer: block6b_se_excite, False\n",
      "Layer: block6b_project_conv, False\n",
      "Layer: block6b_project_bn, False\n",
      "Layer: block6b_drop, False\n",
      "Layer: block6b_add, False\n",
      "Layer: block6c_expand_conv, False\n",
      "Layer: block6c_expand_bn, False\n",
      "Layer: block6c_expand_activation, False\n",
      "Layer: block6c_dwconv, False\n",
      "Layer: block6c_bn, False\n",
      "Layer: block6c_activation, False\n",
      "Layer: block6c_se_squeeze, False\n",
      "Layer: block6c_se_reshape, False\n",
      "Layer: block6c_se_reduce, False\n",
      "Layer: block6c_se_expand, False\n",
      "Layer: block6c_se_excite, False\n",
      "Layer: block6c_project_conv, False\n",
      "Layer: block6c_project_bn, False\n",
      "Layer: block6c_drop, False\n",
      "Layer: block6c_add, False\n",
      "Layer: block6d_expand_conv, False\n",
      "Layer: block6d_expand_bn, False\n",
      "Layer: block6d_expand_activation, False\n",
      "Layer: block6d_dwconv, False\n",
      "Layer: block6d_bn, False\n",
      "Layer: block6d_activation, False\n",
      "Layer: block6d_se_squeeze, False\n",
      "Layer: block6d_se_reshape, False\n",
      "Layer: block6d_se_reduce, False\n",
      "Layer: block6d_se_expand, False\n",
      "Layer: block6d_se_excite, False\n",
      "Layer: block6d_project_conv, False\n",
      "Layer: block6d_project_bn, False\n",
      "Layer: block6d_drop, False\n",
      "Layer: block6d_add, False\n",
      "Layer: block6e_expand_conv, False\n",
      "Layer: block6e_expand_bn, False\n",
      "Layer: block6e_expand_activation, False\n",
      "Layer: block6e_dwconv, False\n",
      "Layer: block6e_bn, False\n",
      "Layer: block6e_activation, False\n",
      "Layer: block6e_se_squeeze, False\n",
      "Layer: block6e_se_reshape, False\n",
      "Layer: block6e_se_reduce, False\n",
      "Layer: block6e_se_expand, False\n",
      "Layer: block6e_se_excite, False\n",
      "Layer: block6e_project_conv, False\n",
      "Layer: block6e_project_bn, False\n",
      "Layer: block6e_drop, False\n",
      "Layer: block6e_add, False\n",
      "Layer: block6f_expand_conv, False\n",
      "Layer: block6f_expand_bn, False\n",
      "Layer: block6f_expand_activation, False\n",
      "Layer: block6f_dwconv, False\n",
      "Layer: block6f_bn, False\n",
      "Layer: block6f_activation, False\n",
      "Layer: block6f_se_squeeze, False\n",
      "Layer: block6f_se_reshape, False\n",
      "Layer: block6f_se_reduce, False\n",
      "Layer: block6f_se_expand, False\n",
      "Layer: block6f_se_excite, False\n",
      "Layer: block6f_project_conv, False\n",
      "Layer: block6f_project_bn, False\n",
      "Layer: block6f_drop, False\n",
      "Layer: block6f_add, False\n",
      "Layer: block6g_expand_conv, False\n",
      "Layer: block6g_expand_bn, False\n",
      "Layer: block6g_expand_activation, False\n",
      "Layer: block6g_dwconv, False\n",
      "Layer: block6g_bn, False\n",
      "Layer: block6g_activation, False\n",
      "Layer: block6g_se_squeeze, False\n",
      "Layer: block6g_se_reshape, False\n",
      "Layer: block6g_se_reduce, False\n",
      "Layer: block6g_se_expand, False\n",
      "Layer: block6g_se_excite, False\n",
      "Layer: block6g_project_conv, False\n",
      "Layer: block6g_project_bn, False\n",
      "Layer: block6g_drop, False\n",
      "Layer: block6g_add, False\n",
      "Layer: block6h_expand_conv, False\n",
      "Layer: block6h_expand_bn, False\n",
      "Layer: block6h_expand_activation, False\n",
      "Layer: block6h_dwconv, False\n",
      "Layer: block6h_bn, False\n",
      "Layer: block6h_activation, False\n",
      "Layer: block6h_se_squeeze, False\n",
      "Layer: block6h_se_reshape, False\n",
      "Layer: block6h_se_reduce, False\n",
      "Layer: block6h_se_expand, False\n",
      "Layer: block6h_se_excite, False\n",
      "Layer: block6h_project_conv, False\n",
      "Layer: block6h_project_bn, False\n",
      "Layer: block6h_drop, False\n",
      "Layer: block6h_add, False\n",
      "Layer: block6i_expand_conv, False\n",
      "Layer: block6i_expand_bn, False\n",
      "Layer: block6i_expand_activation, False\n",
      "Layer: block6i_dwconv, False\n",
      "Layer: block6i_bn, False\n",
      "Layer: block6i_activation, False\n",
      "Layer: block6i_se_squeeze, False\n",
      "Layer: block6i_se_reshape, False\n",
      "Layer: block6i_se_reduce, False\n",
      "Layer: block6i_se_expand, False\n",
      "Layer: block6i_se_excite, False\n",
      "Layer: block6i_project_conv, False\n",
      "Layer: block6i_project_bn, False\n",
      "Layer: block6i_drop, False\n",
      "Layer: block6i_add, False\n",
      "Layer: block6j_expand_conv, False\n",
      "Layer: block6j_expand_bn, False\n",
      "Layer: block6j_expand_activation, False\n",
      "Layer: block6j_dwconv, False\n",
      "Layer: block6j_bn, False\n",
      "Layer: block6j_activation, False\n",
      "Layer: block6j_se_squeeze, False\n",
      "Layer: block6j_se_reshape, False\n",
      "Layer: block6j_se_reduce, False\n",
      "Layer: block6j_se_expand, False\n",
      "Layer: block6j_se_excite, False\n",
      "Layer: block6j_project_conv, False\n",
      "Layer: block6j_project_bn, False\n",
      "Layer: block6j_drop, False\n",
      "Layer: block6j_add, False\n",
      "Layer: block6k_expand_conv, False\n",
      "Layer: block6k_expand_bn, False\n",
      "Layer: block6k_expand_activation, False\n",
      "Layer: block6k_dwconv, False\n",
      "Layer: block6k_bn, False\n",
      "Layer: block6k_activation, False\n",
      "Layer: block6k_se_squeeze, False\n",
      "Layer: block6k_se_reshape, False\n",
      "Layer: block6k_se_reduce, False\n",
      "Layer: block6k_se_expand, False\n",
      "Layer: block6k_se_excite, False\n",
      "Layer: block6k_project_conv, False\n",
      "Layer: block6k_project_bn, False\n",
      "Layer: block6k_drop, False\n",
      "Layer: block6k_add, False\n",
      "Layer: block6l_expand_conv, False\n",
      "Layer: block6l_expand_bn, False\n",
      "Layer: block6l_expand_activation, False\n",
      "Layer: block6l_dwconv, False\n",
      "Layer: block6l_bn, False\n",
      "Layer: block6l_activation, False\n",
      "Layer: block6l_se_squeeze, False\n",
      "Layer: block6l_se_reshape, False\n",
      "Layer: block6l_se_reduce, False\n",
      "Layer: block6l_se_expand, False\n",
      "Layer: block6l_se_excite, False\n",
      "Layer: block6l_project_conv, False\n",
      "Layer: block6l_project_bn, False\n",
      "Layer: block6l_drop, False\n",
      "Layer: block6l_add, False\n",
      "Layer: block6m_expand_conv, False\n",
      "Layer: block6m_expand_bn, False\n",
      "Layer: block6m_expand_activation, False\n",
      "Layer: block6m_dwconv, False\n",
      "Layer: block6m_bn, False\n",
      "Layer: block6m_activation, False\n",
      "Layer: block6m_se_squeeze, False\n",
      "Layer: block6m_se_reshape, False\n",
      "Layer: block6m_se_reduce, False\n",
      "Layer: block6m_se_expand, False\n",
      "Layer: block6m_se_excite, False\n",
      "Layer: block6m_project_conv, False\n",
      "Layer: block6m_project_bn, False\n",
      "Layer: block6m_drop, False\n",
      "Layer: block6m_add, False\n",
      "Layer: block7a_expand_conv, False\n",
      "Layer: block7a_expand_bn, False\n",
      "Layer: block7a_expand_activation, False\n",
      "Layer: block7a_dwconv, False\n",
      "Layer: block7a_bn, False\n",
      "Layer: block7a_activation, False\n",
      "Layer: block7a_se_squeeze, False\n",
      "Layer: block7a_se_reshape, False\n",
      "Layer: block7a_se_reduce, False\n",
      "Layer: block7a_se_expand, False\n",
      "Layer: block7a_se_excite, False\n",
      "Layer: block7a_project_conv, False\n",
      "Layer: block7a_project_bn, False\n",
      "Layer: block7b_expand_conv, False\n",
      "Layer: block7b_expand_bn, False\n",
      "Layer: block7b_expand_activation, False\n",
      "Layer: block7b_dwconv, False\n",
      "Layer: block7b_bn, False\n",
      "Layer: block7b_activation, False\n",
      "Layer: block7b_se_squeeze, False\n",
      "Layer: block7b_se_reshape, False\n",
      "Layer: block7b_se_reduce, False\n",
      "Layer: block7b_se_expand, False\n",
      "Layer: block7b_se_excite, False\n",
      "Layer: block7b_project_conv, False\n",
      "Layer: block7b_project_bn, False\n",
      "Layer: block7b_drop, False\n",
      "Layer: block7b_add, False\n",
      "Layer: block7c_expand_conv, False\n",
      "Layer: block7c_expand_bn, False\n",
      "Layer: block7c_expand_activation, False\n",
      "Layer: block7c_dwconv, False\n",
      "Layer: block7c_bn, False\n",
      "Layer: block7c_activation, False\n",
      "Layer: block7c_se_squeeze, False\n",
      "Layer: block7c_se_reshape, False\n",
      "Layer: block7c_se_reduce, False\n",
      "Layer: block7c_se_expand, False\n",
      "Layer: block7c_se_excite, False\n",
      "Layer: block7c_project_conv, False\n",
      "Layer: block7c_project_bn, False\n",
      "Layer: block7c_drop, False\n",
      "Layer: block7c_add, False\n",
      "Layer: block7d_expand_conv, False\n",
      "Layer: block7d_expand_bn, False\n",
      "Layer: block7d_expand_activation, False\n",
      "Layer: block7d_dwconv, False\n",
      "Layer: block7d_bn, False\n",
      "Layer: block7d_activation, False\n",
      "Layer: block7d_se_squeeze, False\n",
      "Layer: block7d_se_reshape, False\n",
      "Layer: block7d_se_reduce, False\n",
      "Layer: block7d_se_expand, False\n",
      "Layer: block7d_se_excite, False\n",
      "Layer: block7d_project_conv, False\n",
      "Layer: block7d_project_bn, False\n",
      "Layer: block7d_drop, False\n",
      "Layer: block7d_add, False\n",
      "Layer: top_conv, False\n",
      "Layer: top_bn, False\n",
      "Layer: top_activation, False\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:33:49.439374Z",
     "start_time": "2024-07-27T00:33:49.435384Z"
    }
   },
   "cell_type": "code",
   "source": "len(base_model.layers)",
   "id": "4d5901824515235e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:33:50.495051Z",
     "start_time": "2024-07-27T00:33:50.491522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "No_of_classes = len(os.listdir(fundus_train))\n",
    "No_of_classes"
   ],
   "id": "c24aa2a52df99eca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:35:50.672948Z",
     "start_time": "2024-07-27T00:35:50.667877Z"
    }
   },
   "cell_type": "code",
   "source": "aug_layer = utils.return_data_aug_layer_for_eff_net(random_width=0.3, random_height=0.3, random_zoom=0.3, random_rotation=0.3)",
   "id": "8f631881607b5da6",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:37:35.802409Z",
     "start_time": "2024-07-27T00:37:34.479020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.layers import Dense, GlobalAvgPool2D, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3), name='Input_layer')\n",
    "x = aug_layer(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = GlobalAvgPool2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(512, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(64, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(32, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(No_of_classes, activation='softmax', dtype=tf.float32)(x)\n",
    "\n",
    "model_1 = Model(inputs, outputs, name='EfficientB7')\n",
    "\n",
    "model_1.summary()"
   ],
   "id": "f8314db62f9c88ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 1509796 (5.76 MB)\n",
      "Non-trainable params: 64105047 (244.54 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:37:39.469016Z",
     "start_time": "2024-07-27T00:37:39.465507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "model_1chkpt = ModelCheckpoint(filepath = os.path.join('Trained_Models',model_1.name), save_weights_only = False, save_best_only = True, verbose = 1)"
   ],
   "id": "a30cb8302eb2cd34",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:37:41.094999Z",
     "start_time": "2024-07-27T00:37:41.092546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.3, patience=2) "
   ],
   "id": "48463ce3a6a20776",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "def save_history(history, name):\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(f'{name}/{name}.csv', index=False)"
   ],
   "id": "4fd4092fa0825dea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T00:37:42.511511Z",
     "start_time": "2024-07-27T00:37:42.494487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model compilation\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])"
   ],
   "id": "18264d9b28fa328f",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:01:16.904477Z",
     "start_time": "2024-07-27T00:37:47.159398Z"
    }
   },
   "cell_type": "code",
   "source": "history_1 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = 15, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "85e1fa6dc6835069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 20:37:57.734920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2024-07-26 20:37:57.970260: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-07-26 20:37:59.435811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x713e500034c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-07-26 20:37:59.435850: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
      "2024-07-26 20:37:59.449912: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-07-26 20:37:59.547366: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - ETA: 0s - loss: 16.0270 - accuracy: 0.3800\n",
      "Epoch 1: val_loss improved from inf to 13.50389, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 125s 2s/step - loss: 16.0270 - accuracy: 0.3800 - val_loss: 13.5039 - val_accuracy: 0.6186 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 11.1185 - accuracy: 0.5406\n",
      "Epoch 2: val_loss improved from 13.50389 to 9.16190, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 112s 2s/step - loss: 11.1185 - accuracy: 0.5406 - val_loss: 9.1619 - val_accuracy: 0.1153 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 7.4122 - accuracy: 0.5949\n",
      "Epoch 3: val_loss improved from 9.16190 to 6.34113, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 110s 2s/step - loss: 7.4122 - accuracy: 0.5949 - val_loss: 6.3411 - val_accuracy: 0.2220 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 5.1401 - accuracy: 0.6419\n",
      "Epoch 4: val_loss improved from 6.34113 to 4.67035, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 106s 2s/step - loss: 5.1401 - accuracy: 0.6419 - val_loss: 4.6703 - val_accuracy: 0.3017 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 3.8682 - accuracy: 0.6410\n",
      "Epoch 5: val_loss improved from 4.67035 to 3.59066, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 108s 2s/step - loss: 3.8682 - accuracy: 0.6410 - val_loss: 3.5907 - val_accuracy: 0.3983 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 3.0096 - accuracy: 0.6715\n",
      "Epoch 6: val_loss improved from 3.59066 to 2.93700, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 104s 2s/step - loss: 3.0096 - accuracy: 0.6715 - val_loss: 2.9370 - val_accuracy: 0.4034 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 2.4680 - accuracy: 0.6724\n",
      "Epoch 7: val_loss improved from 2.93700 to 2.54391, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 102s 1s/step - loss: 2.4680 - accuracy: 0.6724 - val_loss: 2.5439 - val_accuracy: 0.3492 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 2.1233 - accuracy: 0.6825\n",
      "Epoch 8: val_loss improved from 2.54391 to 2.27184, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 103s 1s/step - loss: 2.1233 - accuracy: 0.6825 - val_loss: 2.2718 - val_accuracy: 0.3720 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.8509 - accuracy: 0.6629\n",
      "Epoch 9: val_loss improved from 2.27184 to 1.87486, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 103s 2s/step - loss: 1.8509 - accuracy: 0.6629 - val_loss: 1.8749 - val_accuracy: 0.5958 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.6363 - accuracy: 0.6775\n",
      "Epoch 10: val_loss improved from 1.87486 to 1.85151, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 111s 2s/step - loss: 1.6363 - accuracy: 0.6775 - val_loss: 1.8515 - val_accuracy: 0.4220 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.5399 - accuracy: 0.6848\n",
      "Epoch 11: val_loss improved from 1.85151 to 1.43217, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 111s 2s/step - loss: 1.5399 - accuracy: 0.6848 - val_loss: 1.4322 - val_accuracy: 0.7695 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.4352 - accuracy: 0.6848\n",
      "Epoch 12: val_loss improved from 1.43217 to 1.27513, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 111s 2s/step - loss: 1.4352 - accuracy: 0.6848 - val_loss: 1.2751 - val_accuracy: 0.8025 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.3811 - accuracy: 0.6925\n",
      "Epoch 13: val_loss did not improve from 1.27513\n",
      "69/69 [==============================] - 36s 513ms/step - loss: 1.3811 - accuracy: 0.6925 - val_loss: 1.3347 - val_accuracy: 0.7288 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.2877 - accuracy: 0.6980\n",
      "Epoch 14: val_loss did not improve from 1.27513\n",
      "69/69 [==============================] - 32s 451ms/step - loss: 1.2877 - accuracy: 0.6980 - val_loss: 1.4491 - val_accuracy: 0.5432 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.1813 - accuracy: 0.7190\n",
      "Epoch 15: val_loss did not improve from 1.27513\n",
      "69/69 [==============================] - 36s 520ms/step - loss: 1.1813 - accuracy: 0.7190 - val_loss: 1.3868 - val_accuracy: 0.5356 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:02:30.637525Z",
     "start_time": "2024-07-27T01:02:30.635319Z"
    }
   },
   "cell_type": "code",
   "source": "start_epoch = 15",
   "id": "697881228f407aa3",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:09:29.664939Z",
     "start_time": "2024-07-27T01:03:08.555988Z"
    }
   },
   "cell_type": "code",
   "source": "history_2 = model_1.fit(train_datagen, validation_data = (val_datagen), initial_epoch= start_epoch, epochs = start_epoch+5 ,verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "ce336cb30d76cc83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.0862 - accuracy: 0.7322\n",
      "Epoch 16: val_loss improved from 1.27513 to 1.26643, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 106s 2s/step - loss: 1.0862 - accuracy: 0.7322 - val_loss: 1.2664 - val_accuracy: 0.5864 - lr: 3.0000e-04\n",
      "Epoch 17/20\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.0267 - accuracy: 0.7427\n",
      "Epoch 17: val_loss improved from 1.26643 to 1.09890, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 106s 2s/step - loss: 1.0267 - accuracy: 0.7427 - val_loss: 1.0989 - val_accuracy: 0.7119 - lr: 3.0000e-04\n",
      "Epoch 18/20\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9917 - accuracy: 0.7518\n",
      "Epoch 18: val_loss did not improve from 1.09890\n",
      "69/69 [==============================] - 32s 459ms/step - loss: 0.9917 - accuracy: 0.7518 - val_loss: 1.1143 - val_accuracy: 0.7102 - lr: 3.0000e-04\n",
      "Epoch 19/20\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9891 - accuracy: 0.7582\n",
      "Epoch 19: val_loss improved from 1.09890 to 0.90306, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 105s 2s/step - loss: 0.9891 - accuracy: 0.7582 - val_loss: 0.9031 - val_accuracy: 0.8297 - lr: 3.0000e-04\n",
      "Epoch 20/20\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.0188 - accuracy: 0.7491\n",
      "Epoch 20: val_loss did not improve from 0.90306\n",
      "69/69 [==============================] - 31s 448ms/step - loss: 1.0188 - accuracy: 0.7491 - val_loss: 1.0551 - val_accuracy: 0.6788 - lr: 3.0000e-04\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:10:19.800809Z",
     "start_time": "2024-07-27T01:10:19.798747Z"
    }
   },
   "cell_type": "code",
   "source": "start_epoch = 20",
   "id": "45cc7cf9372a66c3",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:21:52.885311Z",
     "start_time": "2024-07-27T01:10:20.202065Z"
    }
   },
   "cell_type": "code",
   "source": "history_3 = model_1.fit(train_datagen, validation_data = (val_datagen), initial_epoch= start_epoch, epochs = start_epoch+15 ,verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "62b3cf24405551e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9876 - accuracy: 0.7477\n",
      "Epoch 21: val_loss did not improve from 0.90306\n",
      "69/69 [==============================] - 30s 433ms/step - loss: 0.9876 - accuracy: 0.7477 - val_loss: 1.1372 - val_accuracy: 0.6127 - lr: 3.0000e-04\n",
      "Epoch 22/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9820 - accuracy: 0.7464\n",
      "Epoch 22: val_loss improved from 0.90306 to 0.86742, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 107s 2s/step - loss: 0.9820 - accuracy: 0.7464 - val_loss: 0.8674 - val_accuracy: 0.7983 - lr: 3.0000e-04\n",
      "Epoch 23/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9444 - accuracy: 0.7541\n",
      "Epoch 23: val_loss did not improve from 0.86742\n",
      "69/69 [==============================] - 31s 448ms/step - loss: 0.9444 - accuracy: 0.7541 - val_loss: 0.9320 - val_accuracy: 0.7483 - lr: 3.0000e-04\n",
      "Epoch 24/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9231 - accuracy: 0.7692\n",
      "Epoch 24: val_loss did not improve from 0.86742\n",
      "69/69 [==============================] - 30s 430ms/step - loss: 0.9231 - accuracy: 0.7692 - val_loss: 1.3247 - val_accuracy: 0.5144 - lr: 3.0000e-04\n",
      "Epoch 25/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9142 - accuracy: 0.7760\n",
      "Epoch 25: val_loss did not improve from 0.86742\n",
      "69/69 [==============================] - 29s 421ms/step - loss: 0.9142 - accuracy: 0.7760 - val_loss: 0.9622 - val_accuracy: 0.7237 - lr: 9.0000e-05\n",
      "Epoch 26/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8810 - accuracy: 0.7742\n",
      "Epoch 26: val_loss did not improve from 0.86742\n",
      "69/69 [==============================] - 27s 392ms/step - loss: 0.8810 - accuracy: 0.7742 - val_loss: 0.9312 - val_accuracy: 0.7212 - lr: 9.0000e-05\n",
      "Epoch 27/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8026 - accuracy: 0.8066\n",
      "Epoch 27: val_loss did not improve from 0.86742\n",
      "69/69 [==============================] - 31s 442ms/step - loss: 0.8026 - accuracy: 0.8066 - val_loss: 0.9230 - val_accuracy: 0.7136 - lr: 2.7000e-05\n",
      "Epoch 28/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8075 - accuracy: 0.8011\n",
      "Epoch 28: val_loss improved from 0.86742 to 0.80573, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 102s 1s/step - loss: 0.8075 - accuracy: 0.8011 - val_loss: 0.8057 - val_accuracy: 0.7915 - lr: 2.7000e-05\n",
      "Epoch 29/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7940 - accuracy: 0.8084\n",
      "Epoch 29: val_loss improved from 0.80573 to 0.73500, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 107s 2s/step - loss: 0.7940 - accuracy: 0.8084 - val_loss: 0.7350 - val_accuracy: 0.8305 - lr: 2.7000e-05\n",
      "Epoch 30/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7531 - accuracy: 0.8166\n",
      "Epoch 30: val_loss did not improve from 0.73500\n",
      "69/69 [==============================] - 33s 476ms/step - loss: 0.7531 - accuracy: 0.8166 - val_loss: 0.8048 - val_accuracy: 0.7890 - lr: 2.7000e-05\n",
      "Epoch 31/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7541 - accuracy: 0.8130\n",
      "Epoch 31: val_loss did not improve from 0.73500\n",
      "69/69 [==============================] - 33s 473ms/step - loss: 0.7541 - accuracy: 0.8130 - val_loss: 0.9162 - val_accuracy: 0.7042 - lr: 2.7000e-05\n",
      "Epoch 32/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7596 - accuracy: 0.8189\n",
      "Epoch 32: val_loss did not improve from 0.73500\n",
      "69/69 [==============================] - 34s 491ms/step - loss: 0.7596 - accuracy: 0.8189 - val_loss: 0.8499 - val_accuracy: 0.7500 - lr: 8.1000e-06\n",
      "Epoch 33/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7351 - accuracy: 0.8180\n",
      "Epoch 33: val_loss did not improve from 0.73500\n",
      "69/69 [==============================] - 32s 465ms/step - loss: 0.7351 - accuracy: 0.8180 - val_loss: 0.8133 - val_accuracy: 0.7695 - lr: 8.1000e-06\n",
      "Epoch 34/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7479 - accuracy: 0.8130\n",
      "Epoch 34: val_loss did not improve from 0.73500\n",
      "69/69 [==============================] - 34s 489ms/step - loss: 0.7479 - accuracy: 0.8130 - val_loss: 0.8164 - val_accuracy: 0.7712 - lr: 2.4300e-06\n",
      "Epoch 35/35\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7254 - accuracy: 0.8257\n",
      "Epoch 35: val_loss did not improve from 0.73500\n",
      "69/69 [==============================] - 32s 455ms/step - loss: 0.7254 - accuracy: 0.8257 - val_loss: 0.7928 - val_accuracy: 0.7805 - lr: 2.4300e-06\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:24:24.213896Z",
     "start_time": "2024-07-27T01:24:24.202214Z"
    }
   },
   "cell_type": "code",
   "source": "base_model.trainable = True",
   "id": "9b7e0fe022b2e5ad",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:24:26.306578Z",
     "start_time": "2024-07-27T01:24:26.294406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers[:-25]:\n",
    "    layer.trainable = False"
   ],
   "id": "144c133d132993be",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:24:28.188792Z",
     "start_time": "2024-07-27T01:24:28.183458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers:\n",
    "    print(layer.trainable)"
   ],
   "id": "a853d58f3af9fa71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:24:45.327916Z",
     "start_time": "2024-07-27T01:24:45.254334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 35\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0001), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "7233597bf79998e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 13044196 (49.76 MB)\n",
      "Non-trainable params: 52570647 (200.54 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:36:03.977813Z",
     "start_time": "2024-07-27T01:25:05.606280Z"
    }
   },
   "cell_type": "code",
   "source": "history_25 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "eeb24a49a8759ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7974 - accuracy: 0.7924\n",
      "Epoch 36: val_loss improved from 0.73500 to 0.72698, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 119s 2s/step - loss: 0.7974 - accuracy: 0.7924 - val_loss: 0.7270 - val_accuracy: 0.8212 - lr: 1.0000e-04\n",
      "Epoch 37/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7720 - accuracy: 0.8029\n",
      "Epoch 37: val_loss did not improve from 0.72698\n",
      "69/69 [==============================] - 27s 387ms/step - loss: 0.7720 - accuracy: 0.8029 - val_loss: 1.0841 - val_accuracy: 0.6025 - lr: 1.0000e-04\n",
      "Epoch 38/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7863 - accuracy: 0.8034\n",
      "Epoch 38: val_loss improved from 0.72698 to 0.68241, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 99s 1s/step - loss: 0.7863 - accuracy: 0.8034 - val_loss: 0.6824 - val_accuracy: 0.8322 - lr: 1.0000e-04\n",
      "Epoch 39/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7198 - accuracy: 0.8180\n",
      "Epoch 39: val_loss did not improve from 0.68241\n",
      "69/69 [==============================] - 28s 400ms/step - loss: 0.7198 - accuracy: 0.8180 - val_loss: 0.7971 - val_accuracy: 0.7839 - lr: 1.0000e-04\n",
      "Epoch 40/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7110 - accuracy: 0.8221\n",
      "Epoch 40: val_loss improved from 0.68241 to 0.64924, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 102s 1s/step - loss: 0.7110 - accuracy: 0.8221 - val_loss: 0.6492 - val_accuracy: 0.8314 - lr: 1.0000e-04\n",
      "Epoch 41/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7416 - accuracy: 0.8057\n",
      "Epoch 41: val_loss did not improve from 0.64924\n",
      "69/69 [==============================] - 28s 405ms/step - loss: 0.7416 - accuracy: 0.8057 - val_loss: 0.8600 - val_accuracy: 0.6958 - lr: 1.0000e-04\n",
      "Epoch 42/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.7032 - accuracy: 0.8262\n",
      "Epoch 42: val_loss improved from 0.64924 to 0.55353, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 103s 2s/step - loss: 0.7032 - accuracy: 0.8262 - val_loss: 0.5535 - val_accuracy: 0.8712 - lr: 1.0000e-04\n",
      "Epoch 43/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6785 - accuracy: 0.8317\n",
      "Epoch 43: val_loss did not improve from 0.55353\n",
      "69/69 [==============================] - 26s 377ms/step - loss: 0.6785 - accuracy: 0.8317 - val_loss: 0.7578 - val_accuracy: 0.7780 - lr: 1.0000e-04\n",
      "Epoch 44/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6833 - accuracy: 0.8271\n",
      "Epoch 44: val_loss did not improve from 0.55353\n",
      "69/69 [==============================] - 24s 342ms/step - loss: 0.6833 - accuracy: 0.8271 - val_loss: 0.6081 - val_accuracy: 0.8483 - lr: 1.0000e-04\n",
      "Epoch 45/45\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6737 - accuracy: 0.8312\n",
      "Epoch 45: val_loss improved from 0.55353 to 0.53355, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 102s 1s/step - loss: 0.6737 - accuracy: 0.8312 - val_loss: 0.5336 - val_accuracy: 0.8924 - lr: 3.0000e-05\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:38:01.394167Z",
     "start_time": "2024-07-27T01:38:01.390959Z"
    }
   },
   "cell_type": "code",
   "source": "start_epoch = 45",
   "id": "90ea8d2c28823587",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:50:08.295078Z",
     "start_time": "2024-07-27T01:38:15.379811Z"
    }
   },
   "cell_type": "code",
   "source": "history_45 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "d0ded9ab1e9786d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6276 - accuracy: 0.8444\n",
      "Epoch 46: val_loss improved from 0.53355 to 0.52745, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 103s 2s/step - loss: 0.6276 - accuracy: 0.8444 - val_loss: 0.5274 - val_accuracy: 0.8966 - lr: 3.0000e-05\n",
      "Epoch 47/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5986 - accuracy: 0.8590\n",
      "Epoch 47: val_loss improved from 0.52745 to 0.50742, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 106s 2s/step - loss: 0.5986 - accuracy: 0.8590 - val_loss: 0.5074 - val_accuracy: 0.9008 - lr: 3.0000e-05\n",
      "Epoch 48/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5757 - accuracy: 0.8627\n",
      "Epoch 48: val_loss improved from 0.50742 to 0.49251, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 108s 2s/step - loss: 0.5757 - accuracy: 0.8627 - val_loss: 0.4925 - val_accuracy: 0.9025 - lr: 3.0000e-05\n",
      "Epoch 49/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.8782\n",
      "Epoch 49: val_loss did not improve from 0.49251\n",
      "69/69 [==============================] - 25s 355ms/step - loss: 0.5605 - accuracy: 0.8782 - val_loss: 0.5018 - val_accuracy: 0.9102 - lr: 3.0000e-05\n",
      "Epoch 50/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5634 - accuracy: 0.8709\n",
      "Epoch 50: val_loss did not improve from 0.49251\n",
      "69/69 [==============================] - 25s 359ms/step - loss: 0.5634 - accuracy: 0.8709 - val_loss: 0.5941 - val_accuracy: 0.8559 - lr: 3.0000e-05\n",
      "Epoch 51/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5582 - accuracy: 0.8695\n",
      "Epoch 51: val_loss improved from 0.49251 to 0.47443, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 96s 1s/step - loss: 0.5582 - accuracy: 0.8695 - val_loss: 0.4744 - val_accuracy: 0.8992 - lr: 9.0000e-06\n",
      "Epoch 52/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5322 - accuracy: 0.8878\n",
      "Epoch 52: val_loss did not improve from 0.47443\n",
      "69/69 [==============================] - 24s 337ms/step - loss: 0.5322 - accuracy: 0.8878 - val_loss: 0.4926 - val_accuracy: 0.8966 - lr: 9.0000e-06\n",
      "Epoch 53/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5438 - accuracy: 0.8700\n",
      "Epoch 53: val_loss improved from 0.47443 to 0.45179, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 98s 1s/step - loss: 0.5438 - accuracy: 0.8700 - val_loss: 0.4518 - val_accuracy: 0.9093 - lr: 9.0000e-06\n",
      "Epoch 54/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5456 - accuracy: 0.8750\n",
      "Epoch 54: val_loss did not improve from 0.45179\n",
      "69/69 [==============================] - 24s 342ms/step - loss: 0.5456 - accuracy: 0.8750 - val_loss: 0.4769 - val_accuracy: 0.9017 - lr: 9.0000e-06\n",
      "Epoch 55/55\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5323 - accuracy: 0.8768\n",
      "Epoch 55: val_loss improved from 0.45179 to 0.44409, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 104s 2s/step - loss: 0.5323 - accuracy: 0.8768 - val_loss: 0.4441 - val_accuracy: 0.9068 - lr: 9.0000e-06\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:51:08.945151Z",
     "start_time": "2024-07-27T01:51:08.940436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 55\n",
    "for layer in base_model.layers[-75:]:\n",
    "    layer.trainable = True"
   ],
   "id": "6a7cd8da92ae1b49",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:51:13.086973Z",
     "start_time": "2024-07-27T01:51:12.999105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "1e494e7e29768448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 25983300 (99.12 MB)\n",
      "Non-trainable params: 39631543 (151.18 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:55:45.362176Z",
     "start_time": "2024-07-27T01:51:16.682097Z"
    }
   },
   "cell_type": "code",
   "source": "history_75 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "81d7f639996a2cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 1.2520 - accuracy: 0.6770\n",
      "Epoch 56: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 47s 439ms/step - loss: 1.2520 - accuracy: 0.6770 - val_loss: 12.5162 - val_accuracy: 0.1559 - lr: 0.0010\n",
      "Epoch 57/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9120 - accuracy: 0.7400\n",
      "Epoch 57: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 24s 348ms/step - loss: 0.9120 - accuracy: 0.7400 - val_loss: 5.5528 - val_accuracy: 0.0712 - lr: 0.0010\n",
      "Epoch 58/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8669 - accuracy: 0.7582\n",
      "Epoch 58: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 25s 360ms/step - loss: 0.8669 - accuracy: 0.7582 - val_loss: 0.5624 - val_accuracy: 0.8881 - lr: 0.0010\n",
      "Epoch 59/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9088 - accuracy: 0.7322\n",
      "Epoch 59: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 24s 346ms/step - loss: 0.9088 - accuracy: 0.7322 - val_loss: 70.0049 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 60/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.9835 - accuracy: 0.7409\n",
      "Epoch 60: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 26s 377ms/step - loss: 0.9835 - accuracy: 0.7409 - val_loss: 8.1757 - val_accuracy: 0.1288 - lr: 0.0010\n",
      "Epoch 61/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.8088 - accuracy: 0.7797\n",
      "Epoch 61: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 23s 324ms/step - loss: 0.8088 - accuracy: 0.7797 - val_loss: 0.7337 - val_accuracy: 0.8525 - lr: 3.0000e-04\n",
      "Epoch 62/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6642 - accuracy: 0.8216\n",
      "Epoch 62: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 24s 341ms/step - loss: 0.6642 - accuracy: 0.8216 - val_loss: 1.0109 - val_accuracy: 0.6458 - lr: 3.0000e-04\n",
      "Epoch 63/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.8289\n",
      "Epoch 63: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 25s 362ms/step - loss: 0.6521 - accuracy: 0.8289 - val_loss: 0.4568 - val_accuracy: 0.9246 - lr: 9.0000e-05\n",
      "Epoch 64/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.8526\n",
      "Epoch 64: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 25s 359ms/step - loss: 0.5859 - accuracy: 0.8526 - val_loss: 0.5631 - val_accuracy: 0.9017 - lr: 9.0000e-05\n",
      "Epoch 65/65\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.6084 - accuracy: 0.8403\n",
      "Epoch 65: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 26s 365ms/step - loss: 0.6084 - accuracy: 0.8403 - val_loss: 0.4848 - val_accuracy: 0.9102 - lr: 9.0000e-05\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T01:56:13.198118Z",
     "start_time": "2024-07-27T01:56:13.195629Z"
    }
   },
   "cell_type": "code",
   "source": "start_epoch = 65",
   "id": "e416a52ea7bd009e",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:07:19.297385Z",
     "start_time": "2024-07-27T01:56:14.176818Z"
    }
   },
   "cell_type": "code",
   "source": "history_ = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "96326be835249a4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5598 - accuracy: 0.8435\n",
      "Epoch 66: val_loss did not improve from 0.44409\n",
      "69/69 [==============================] - 24s 342ms/step - loss: 0.5598 - accuracy: 0.8435 - val_loss: 0.4485 - val_accuracy: 0.9237 - lr: 2.7000e-05\n",
      "Epoch 67/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5608 - accuracy: 0.8572\n",
      "Epoch 67: val_loss improved from 0.44409 to 0.42638, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 110s 2s/step - loss: 0.5608 - accuracy: 0.8572 - val_loss: 0.4264 - val_accuracy: 0.9246 - lr: 2.7000e-05\n",
      "Epoch 68/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5595 - accuracy: 0.8499\n",
      "Epoch 68: val_loss did not improve from 0.42638\n",
      "69/69 [==============================] - 24s 346ms/step - loss: 0.5595 - accuracy: 0.8499 - val_loss: 0.4686 - val_accuracy: 0.9144 - lr: 2.7000e-05\n",
      "Epoch 69/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5709 - accuracy: 0.8353\n",
      "Epoch 69: val_loss did not improve from 0.42638\n",
      "69/69 [==============================] - 24s 349ms/step - loss: 0.5709 - accuracy: 0.8353 - val_loss: 0.4533 - val_accuracy: 0.9220 - lr: 2.7000e-05\n",
      "Epoch 70/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.8636\n",
      "Epoch 70: val_loss improved from 0.42638 to 0.40182, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 107s 2s/step - loss: 0.5243 - accuracy: 0.8636 - val_loss: 0.4018 - val_accuracy: 0.9263 - lr: 8.1000e-06\n",
      "Epoch 71/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5189 - accuracy: 0.8663\n",
      "Epoch 71: val_loss did not improve from 0.40182\n",
      "69/69 [==============================] - 25s 367ms/step - loss: 0.5189 - accuracy: 0.8663 - val_loss: 0.4161 - val_accuracy: 0.9237 - lr: 8.1000e-06\n",
      "Epoch 72/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5325 - accuracy: 0.8590\n",
      "Epoch 72: val_loss did not improve from 0.40182\n",
      "69/69 [==============================] - 25s 362ms/step - loss: 0.5325 - accuracy: 0.8590 - val_loss: 0.4112 - val_accuracy: 0.9237 - lr: 8.1000e-06\n",
      "Epoch 73/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5239 - accuracy: 0.8558\n",
      "Epoch 73: val_loss improved from 0.40182 to 0.39940, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 106s 2s/step - loss: 0.5239 - accuracy: 0.8558 - val_loss: 0.3994 - val_accuracy: 0.9263 - lr: 2.4300e-06\n",
      "Epoch 74/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.8609\n",
      "Epoch 74: val_loss improved from 0.39940 to 0.39655, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 118s 2s/step - loss: 0.5291 - accuracy: 0.8609 - val_loss: 0.3966 - val_accuracy: 0.9246 - lr: 2.4300e-06\n",
      "Epoch 75/75\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5384 - accuracy: 0.8568\n",
      "Epoch 75: val_loss improved from 0.39655 to 0.39276, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 101s 1s/step - loss: 0.5384 - accuracy: 0.8568 - val_loss: 0.3928 - val_accuracy: 0.9254 - lr: 2.4300e-06\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:07:24.266803Z",
     "start_time": "2024-07-27T02:07:24.261317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 75\n",
    "for layer in base_model.layers[-150:]:\n",
    "    layer.trainable = True"
   ],
   "id": "9c3fa202d236af9e",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:07:38.451476Z",
     "start_time": "2024-07-27T02:07:38.374101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.000006), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "10b77e72c3b9b02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 37392420 (142.64 MB)\n",
      "Non-trainable params: 28222423 (107.66 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:16:25.272730Z",
     "start_time": "2024-07-27T02:07:44.923653Z"
    }
   },
   "cell_type": "code",
   "source": "history_75 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "38d94ee2c181b1df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.8641\n",
      "Epoch 76: val_loss did not improve from 0.39276\n",
      "69/69 [==============================] - 47s 388ms/step - loss: 0.5404 - accuracy: 0.8641 - val_loss: 0.4107 - val_accuracy: 0.9195 - lr: 6.0000e-06\n",
      "Epoch 77/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.8590\n",
      "Epoch 77: val_loss did not improve from 0.39276\n",
      "69/69 [==============================] - 22s 318ms/step - loss: 0.5375 - accuracy: 0.8590 - val_loss: 0.4017 - val_accuracy: 0.9178 - lr: 6.0000e-06\n",
      "Epoch 78/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.8613\n",
      "Epoch 78: val_loss improved from 0.39276 to 0.38639, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 121s 2s/step - loss: 0.5095 - accuracy: 0.8613 - val_loss: 0.3864 - val_accuracy: 0.9254 - lr: 6.0000e-06\n",
      "Epoch 79/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5262 - accuracy: 0.8663\n",
      "Epoch 79: val_loss improved from 0.38639 to 0.38213, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 98s 1s/step - loss: 0.5262 - accuracy: 0.8663 - val_loss: 0.3821 - val_accuracy: 0.9280 - lr: 6.0000e-06\n",
      "Epoch 80/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.8663\n",
      "Epoch 80: val_loss did not improve from 0.38213\n",
      "69/69 [==============================] - 26s 369ms/step - loss: 0.5125 - accuracy: 0.8663 - val_loss: 0.4008 - val_accuracy: 0.9220 - lr: 6.0000e-06\n",
      "Epoch 81/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5120 - accuracy: 0.8613\n",
      "Epoch 81: val_loss did not improve from 0.38213\n",
      "69/69 [==============================] - 24s 343ms/step - loss: 0.5120 - accuracy: 0.8613 - val_loss: 0.4136 - val_accuracy: 0.9178 - lr: 6.0000e-06\n",
      "Epoch 82/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.8727\n",
      "Epoch 82: val_loss did not improve from 0.38213\n",
      "69/69 [==============================] - 22s 310ms/step - loss: 0.4842 - accuracy: 0.8727 - val_loss: 0.3969 - val_accuracy: 0.9220 - lr: 1.8000e-06\n",
      "Epoch 83/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5099 - accuracy: 0.8672\n",
      "Epoch 83: val_loss did not improve from 0.38213\n",
      "69/69 [==============================] - 22s 322ms/step - loss: 0.5099 - accuracy: 0.8672 - val_loss: 0.3861 - val_accuracy: 0.9229 - lr: 1.8000e-06\n",
      "Epoch 84/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5075 - accuracy: 0.8631\n",
      "Epoch 84: val_loss improved from 0.38213 to 0.38105, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 114s 2s/step - loss: 0.5075 - accuracy: 0.8631 - val_loss: 0.3810 - val_accuracy: 0.9229 - lr: 5.4000e-07\n",
      "Epoch 85/85\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5199 - accuracy: 0.8663\n",
      "Epoch 85: val_loss did not improve from 0.38105\n",
      "69/69 [==============================] - 24s 341ms/step - loss: 0.5199 - accuracy: 0.8663 - val_loss: 0.3826 - val_accuracy: 0.9220 - lr: 5.4000e-07\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:16:57.204882Z",
     "start_time": "2024-07-27T02:16:57.197485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 85\n",
    "for layer in base_model.layers[-250:]:\n",
    "    layer.trainable = True"
   ],
   "id": "f24912d6c3aa128",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:17:43.701856Z",
     "start_time": "2024-07-27T02:17:43.612079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0000008), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "81ac6230c263bbc4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 52639580 (200.80 MB)\n",
      "Non-trainable params: 12975263 (49.50 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:25:20.508470Z",
     "start_time": "2024-07-27T02:17:47.666921Z"
    }
   },
   "cell_type": "code",
   "source": "history_75 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "fd98979b64e68f4e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.8691\n",
      "Epoch 86: val_loss improved from 0.38105 to 0.37959, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 147s 2s/step - loss: 0.5035 - accuracy: 0.8691 - val_loss: 0.3796 - val_accuracy: 0.9237 - lr: 8.0000e-07\n",
      "Epoch 87/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4935 - accuracy: 0.8682\n",
      "Epoch 87: val_loss did not improve from 0.37959\n",
      "69/69 [==============================] - 24s 336ms/step - loss: 0.4935 - accuracy: 0.8682 - val_loss: 0.3799 - val_accuracy: 0.9229 - lr: 8.0000e-07\n",
      "Epoch 88/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.8577\n",
      "Epoch 88: val_loss improved from 0.37959 to 0.37635, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 120s 2s/step - loss: 0.5223 - accuracy: 0.8577 - val_loss: 0.3764 - val_accuracy: 0.9237 - lr: 8.0000e-07\n",
      "Epoch 89/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5100 - accuracy: 0.8695\n",
      "Epoch 89: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 22s 311ms/step - loss: 0.5100 - accuracy: 0.8695 - val_loss: 0.3778 - val_accuracy: 0.9246 - lr: 8.0000e-07\n",
      "Epoch 90/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5056 - accuracy: 0.8691\n",
      "Epoch 90: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 22s 318ms/step - loss: 0.5056 - accuracy: 0.8691 - val_loss: 0.3790 - val_accuracy: 0.9237 - lr: 8.0000e-07\n",
      "Epoch 91/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.8604\n",
      "Epoch 91: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 23s 338ms/step - loss: 0.5114 - accuracy: 0.8604 - val_loss: 0.3775 - val_accuracy: 0.9246 - lr: 2.4000e-07\n",
      "Epoch 92/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5183 - accuracy: 0.8663\n",
      "Epoch 92: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 24s 341ms/step - loss: 0.5183 - accuracy: 0.8663 - val_loss: 0.3802 - val_accuracy: 0.9237 - lr: 2.4000e-07\n",
      "Epoch 93/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4816 - accuracy: 0.8755\n",
      "Epoch 93: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 24s 340ms/step - loss: 0.4816 - accuracy: 0.8755 - val_loss: 0.3813 - val_accuracy: 0.9246 - lr: 7.2000e-08\n",
      "Epoch 94/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5020 - accuracy: 0.8686\n",
      "Epoch 94: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 23s 334ms/step - loss: 0.5020 - accuracy: 0.8686 - val_loss: 0.3800 - val_accuracy: 0.9237 - lr: 7.2000e-08\n",
      "Epoch 95/95\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5268 - accuracy: 0.8568\n",
      "Epoch 95: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 24s 346ms/step - loss: 0.5268 - accuracy: 0.8568 - val_loss: 0.3791 - val_accuracy: 0.9246 - lr: 2.1600e-08\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:25:36.239758Z",
     "start_time": "2024-07-27T02:25:36.230458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 95\n",
    "for layer in base_model.layers[-400:]:\n",
    "    layer.trainable = True"
   ],
   "id": "7a8790f759cfa947",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:25:38.087020Z",
     "start_time": "2024-07-27T02:25:37.940123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.000000002), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "137ce8fad4778faf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 60437308 (230.55 MB)\n",
      "Non-trainable params: 5177535 (19.75 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:32:22.920384Z",
     "start_time": "2024-07-27T02:25:42.518314Z"
    }
   },
   "cell_type": "code",
   "source": "history_75 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "8b355e96d1a47442",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.8704\n",
      "Epoch 96: val_loss did not improve from 0.37635\n",
      "69/69 [==============================] - 69s 500ms/step - loss: 0.5063 - accuracy: 0.8704 - val_loss: 0.3773 - val_accuracy: 0.9254 - lr: 2.0000e-09\n",
      "Epoch 97/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5178 - accuracy: 0.8672\n",
      "Epoch 97: val_loss improved from 0.37635 to 0.37435, saving model to Trained_Models/EfficientB7\n",
      "69/69 [==============================] - 127s 2s/step - loss: 0.5178 - accuracy: 0.8672 - val_loss: 0.3743 - val_accuracy: 0.9254 - lr: 2.0000e-09\n",
      "Epoch 98/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4866 - accuracy: 0.8768\n",
      "Epoch 98: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 27s 384ms/step - loss: 0.4866 - accuracy: 0.8768 - val_loss: 0.3777 - val_accuracy: 0.9246 - lr: 2.0000e-09\n",
      "Epoch 99/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5067 - accuracy: 0.8659\n",
      "Epoch 99: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 25s 361ms/step - loss: 0.5067 - accuracy: 0.8659 - val_loss: 0.3758 - val_accuracy: 0.9254 - lr: 2.0000e-09\n",
      "Epoch 100/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5183 - accuracy: 0.8618\n",
      "Epoch 100: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 26s 373ms/step - loss: 0.5183 - accuracy: 0.8618 - val_loss: 0.3772 - val_accuracy: 0.9246 - lr: 6.0000e-10\n",
      "Epoch 101/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4987 - accuracy: 0.8718\n",
      "Epoch 101: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 25s 355ms/step - loss: 0.4987 - accuracy: 0.8718 - val_loss: 0.3774 - val_accuracy: 0.9246 - lr: 6.0000e-10\n",
      "Epoch 102/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8691\n",
      "Epoch 102: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 24s 340ms/step - loss: 0.4947 - accuracy: 0.8691 - val_loss: 0.3775 - val_accuracy: 0.9237 - lr: 1.8000e-10\n",
      "Epoch 103/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5072 - accuracy: 0.8654\n",
      "Epoch 103: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 24s 351ms/step - loss: 0.5072 - accuracy: 0.8654 - val_loss: 0.3805 - val_accuracy: 0.9246 - lr: 1.8000e-10\n",
      "Epoch 104/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.5221 - accuracy: 0.8609\n",
      "Epoch 104: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 28s 400ms/step - loss: 0.5221 - accuracy: 0.8609 - val_loss: 0.3819 - val_accuracy: 0.9246 - lr: 5.4000e-11\n",
      "Epoch 105/105\n",
      "69/69 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.8745\n",
      "Epoch 105: val_loss did not improve from 0.37435\n",
      "69/69 [==============================] - 25s 360ms/step - loss: 0.4964 - accuracy: 0.8745 - val_loss: 0.3775 - val_accuracy: 0.9246 - lr: 5.4000e-11\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:32:29.177467Z",
     "start_time": "2024-07-27T02:32:29.166100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 105\n",
    "for layer in base_model.layers[-600:]:\n",
    "    layer.trainable = True"
   ],
   "id": "13f0e8c01a11c398",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:32:31.067407Z",
     "start_time": "2024-07-27T02:32:30.879663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.000001), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "86ea0d07f9b7b8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"EfficientB7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " Data_Augmentation (Sequent  (None, None, None, 3)     0         \n",
      " ial)                                                            \n",
      "                                                                 \n",
      " efficientnetb7 (Functional  (None, 7, 7, 2560)        64097687  \n",
      " )                                                               \n",
      "                                                                 \n",
      " global_average_pooling2d_4  (None, 2560)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 2560)              10240     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 512)               1311232   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 132       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65614843 (250.30 MB)\n",
      "Trainable params: 64646388 (246.61 MB)\n",
      "Non-trainable params: 968455 (3.69 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-27T02:32:38.238982Z"
    }
   },
   "cell_type": "code",
   "source": "history_75 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+20, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "9f0571a0e580b4f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/125\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:43:07.808058Z",
     "start_time": "2024-07-27T02:42:44.977219Z"
    }
   },
   "cell_type": "code",
   "source": "effb7_97 = tf.keras.models.load_model(\"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/ML_Models/Trained_Models/EfficientB7\")\n",
   "id": "7f00b820f22fca7a",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-27T02:43:15.641045Z",
     "start_time": "2024-07-27T02:43:07.808949Z"
    }
   },
   "cell_type": "code",
   "source": "effb7_97.evaluate(test_datagen)",
   "id": "ebb33e0640d23929",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:43:10.794602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8600\n",
      "2024-07-26 22:43:10.998546: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 8s 142ms/step - loss: 0.5178 - accuracy: 0.8450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5178388357162476, 0.8449704051017761]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d5842a396d77a5ed"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
