{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:52.199469Z",
     "start_time": "2024-07-29T20:19:52.196877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import utils\n",
    "from tensorflow.keras import mixed_precision\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:52.695700Z",
     "start_time": "2024-07-29T20:19:52.692724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "#Batching using prefetch\n",
    "train_data_casted = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "test_data_casted = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\"\"\""
   ],
   "id": "2bfc02fd3bd68fe0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Batching using prefetch\\ntrain_data_casted = train_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).shuffle(buffer_size = 1000).batch(batch_size = 32).prefetch(buffer_size = tf.data.AUTOTUNE)\\ntest_data_casted = test_data.map(map_func = preprocess_image, num_parallel_calls = tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:52.739734Z",
     "start_time": "2024-07-29T20:19:52.737835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fundus_train = \"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/Dataset/split1/train\"\n",
    "fundus_test = \"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/Dataset/split1/test\"\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n"
   ],
   "id": "76e3a5421f09f74d",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:52.761822Z",
     "start_time": "2024-07-29T20:19:52.759503Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.listdir(fundus_train))",
   "id": "1a2371451891c6ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['glaucoma', 'normal', 'cataract', 'diabetic_retinopathy']\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:52.807606Z",
     "start_time": "2024-07-29T20:19:52.805837Z"
    }
   },
   "cell_type": "code",
   "source": "IMG_HEIGHT, IMG_WIDTH = 224, 224",
   "id": "1f94cba87987258",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:53.019779Z",
     "start_time": "2024-07-29T20:19:52.851458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_train,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=True,\n",
    "    seed=123,\n",
    "    validation_split=0.3,\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_train,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "    validation_split=0.3,\n",
    "    subset='validation'\n",
    ")\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    fundus_test,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    shuffle=False,\n",
    "    seed=123,\n",
    "\n",
    ")"
   ],
   "id": "348e18c639023b17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3372 files belonging to 4 classes.\n",
      "Using 2361 files for training.\n",
      "Found 3372 files belonging to 4 classes.\n",
      "Using 1011 files for validation.\n",
      "Found 845 files belonging to 4 classes.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:53.022826Z",
     "start_time": "2024-07-29T20:19:53.020589Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset",
   "id": "d09dab585b1ca859",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:53.029955Z",
     "start_time": "2024-07-29T20:19:53.023398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Using tf prefetch dataset\n",
    "preprocess_input = tf.keras.applications.inception_v3.preprocess_input"
   ],
   "id": "c66d836742f62b76",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:53.202395Z",
     "start_time": "2024-07-29T20:19:53.185310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datagen = train_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "val_datagen = validation_dataset.map(lambda x, y: (preprocess_input(x), y))\n",
    "test_datagen = test_dataset.map(lambda x, y: (preprocess_input(x), y))"
   ],
   "id": "fe651f3421b86187",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:53.379224Z",
     "start_time": "2024-07-29T20:19:53.376542Z"
    }
   },
   "cell_type": "code",
   "source": "train_datagen",
   "id": "2d604d0924419863",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MapDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:54.538890Z",
     "start_time": "2024-07-29T20:19:53.540563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow.keras.applications as apps\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "base_model = apps.InceptionV3(weights = 'imagenet', include_top = False, input_shape = (IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "base_model.trainable = False"
   ],
   "id": "541ea676b52829f7",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:54.543039Z",
     "start_time": "2024-07-29T20:19:54.539768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "for i in base_model.layers:\n",
    "    print(f'Layer: {i.name}, {i.trainable}')"
   ],
   "id": "79db08db0282846b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_2, False\n",
      "Layer: conv2d_94, False\n",
      "Layer: batch_normalization_99, False\n",
      "Layer: activation_98, False\n",
      "Layer: conv2d_95, False\n",
      "Layer: batch_normalization_100, False\n",
      "Layer: activation_99, False\n",
      "Layer: conv2d_96, False\n",
      "Layer: batch_normalization_101, False\n",
      "Layer: activation_100, False\n",
      "Layer: max_pooling2d_4, False\n",
      "Layer: conv2d_97, False\n",
      "Layer: batch_normalization_102, False\n",
      "Layer: activation_101, False\n",
      "Layer: conv2d_98, False\n",
      "Layer: batch_normalization_103, False\n",
      "Layer: activation_102, False\n",
      "Layer: max_pooling2d_5, False\n",
      "Layer: conv2d_102, False\n",
      "Layer: batch_normalization_107, False\n",
      "Layer: activation_106, False\n",
      "Layer: conv2d_100, False\n",
      "Layer: conv2d_103, False\n",
      "Layer: batch_normalization_105, False\n",
      "Layer: batch_normalization_108, False\n",
      "Layer: activation_104, False\n",
      "Layer: activation_107, False\n",
      "Layer: average_pooling2d_9, False\n",
      "Layer: conv2d_99, False\n",
      "Layer: conv2d_101, False\n",
      "Layer: conv2d_104, False\n",
      "Layer: conv2d_105, False\n",
      "Layer: batch_normalization_104, False\n",
      "Layer: batch_normalization_106, False\n",
      "Layer: batch_normalization_109, False\n",
      "Layer: batch_normalization_110, False\n",
      "Layer: activation_103, False\n",
      "Layer: activation_105, False\n",
      "Layer: activation_108, False\n",
      "Layer: activation_109, False\n",
      "Layer: mixed0, False\n",
      "Layer: conv2d_109, False\n",
      "Layer: batch_normalization_114, False\n",
      "Layer: activation_113, False\n",
      "Layer: conv2d_107, False\n",
      "Layer: conv2d_110, False\n",
      "Layer: batch_normalization_112, False\n",
      "Layer: batch_normalization_115, False\n",
      "Layer: activation_111, False\n",
      "Layer: activation_114, False\n",
      "Layer: average_pooling2d_10, False\n",
      "Layer: conv2d_106, False\n",
      "Layer: conv2d_108, False\n",
      "Layer: conv2d_111, False\n",
      "Layer: conv2d_112, False\n",
      "Layer: batch_normalization_111, False\n",
      "Layer: batch_normalization_113, False\n",
      "Layer: batch_normalization_116, False\n",
      "Layer: batch_normalization_117, False\n",
      "Layer: activation_110, False\n",
      "Layer: activation_112, False\n",
      "Layer: activation_115, False\n",
      "Layer: activation_116, False\n",
      "Layer: mixed1, False\n",
      "Layer: conv2d_116, False\n",
      "Layer: batch_normalization_121, False\n",
      "Layer: activation_120, False\n",
      "Layer: conv2d_114, False\n",
      "Layer: conv2d_117, False\n",
      "Layer: batch_normalization_119, False\n",
      "Layer: batch_normalization_122, False\n",
      "Layer: activation_118, False\n",
      "Layer: activation_121, False\n",
      "Layer: average_pooling2d_11, False\n",
      "Layer: conv2d_113, False\n",
      "Layer: conv2d_115, False\n",
      "Layer: conv2d_118, False\n",
      "Layer: conv2d_119, False\n",
      "Layer: batch_normalization_118, False\n",
      "Layer: batch_normalization_120, False\n",
      "Layer: batch_normalization_123, False\n",
      "Layer: batch_normalization_124, False\n",
      "Layer: activation_117, False\n",
      "Layer: activation_119, False\n",
      "Layer: activation_122, False\n",
      "Layer: activation_123, False\n",
      "Layer: mixed2, False\n",
      "Layer: conv2d_121, False\n",
      "Layer: batch_normalization_126, False\n",
      "Layer: activation_125, False\n",
      "Layer: conv2d_122, False\n",
      "Layer: batch_normalization_127, False\n",
      "Layer: activation_126, False\n",
      "Layer: conv2d_120, False\n",
      "Layer: conv2d_123, False\n",
      "Layer: batch_normalization_125, False\n",
      "Layer: batch_normalization_128, False\n",
      "Layer: activation_124, False\n",
      "Layer: activation_127, False\n",
      "Layer: max_pooling2d_6, False\n",
      "Layer: mixed3, False\n",
      "Layer: conv2d_128, False\n",
      "Layer: batch_normalization_133, False\n",
      "Layer: activation_132, False\n",
      "Layer: conv2d_129, False\n",
      "Layer: batch_normalization_134, False\n",
      "Layer: activation_133, False\n",
      "Layer: conv2d_125, False\n",
      "Layer: conv2d_130, False\n",
      "Layer: batch_normalization_130, False\n",
      "Layer: batch_normalization_135, False\n",
      "Layer: activation_129, False\n",
      "Layer: activation_134, False\n",
      "Layer: conv2d_126, False\n",
      "Layer: conv2d_131, False\n",
      "Layer: batch_normalization_131, False\n",
      "Layer: batch_normalization_136, False\n",
      "Layer: activation_130, False\n",
      "Layer: activation_135, False\n",
      "Layer: average_pooling2d_12, False\n",
      "Layer: conv2d_124, False\n",
      "Layer: conv2d_127, False\n",
      "Layer: conv2d_132, False\n",
      "Layer: conv2d_133, False\n",
      "Layer: batch_normalization_129, False\n",
      "Layer: batch_normalization_132, False\n",
      "Layer: batch_normalization_137, False\n",
      "Layer: batch_normalization_138, False\n",
      "Layer: activation_128, False\n",
      "Layer: activation_131, False\n",
      "Layer: activation_136, False\n",
      "Layer: activation_137, False\n",
      "Layer: mixed4, False\n",
      "Layer: conv2d_138, False\n",
      "Layer: batch_normalization_143, False\n",
      "Layer: activation_142, False\n",
      "Layer: conv2d_139, False\n",
      "Layer: batch_normalization_144, False\n",
      "Layer: activation_143, False\n",
      "Layer: conv2d_135, False\n",
      "Layer: conv2d_140, False\n",
      "Layer: batch_normalization_140, False\n",
      "Layer: batch_normalization_145, False\n",
      "Layer: activation_139, False\n",
      "Layer: activation_144, False\n",
      "Layer: conv2d_136, False\n",
      "Layer: conv2d_141, False\n",
      "Layer: batch_normalization_141, False\n",
      "Layer: batch_normalization_146, False\n",
      "Layer: activation_140, False\n",
      "Layer: activation_145, False\n",
      "Layer: average_pooling2d_13, False\n",
      "Layer: conv2d_134, False\n",
      "Layer: conv2d_137, False\n",
      "Layer: conv2d_142, False\n",
      "Layer: conv2d_143, False\n",
      "Layer: batch_normalization_139, False\n",
      "Layer: batch_normalization_142, False\n",
      "Layer: batch_normalization_147, False\n",
      "Layer: batch_normalization_148, False\n",
      "Layer: activation_138, False\n",
      "Layer: activation_141, False\n",
      "Layer: activation_146, False\n",
      "Layer: activation_147, False\n",
      "Layer: mixed5, False\n",
      "Layer: conv2d_148, False\n",
      "Layer: batch_normalization_153, False\n",
      "Layer: activation_152, False\n",
      "Layer: conv2d_149, False\n",
      "Layer: batch_normalization_154, False\n",
      "Layer: activation_153, False\n",
      "Layer: conv2d_145, False\n",
      "Layer: conv2d_150, False\n",
      "Layer: batch_normalization_150, False\n",
      "Layer: batch_normalization_155, False\n",
      "Layer: activation_149, False\n",
      "Layer: activation_154, False\n",
      "Layer: conv2d_146, False\n",
      "Layer: conv2d_151, False\n",
      "Layer: batch_normalization_151, False\n",
      "Layer: batch_normalization_156, False\n",
      "Layer: activation_150, False\n",
      "Layer: activation_155, False\n",
      "Layer: average_pooling2d_14, False\n",
      "Layer: conv2d_144, False\n",
      "Layer: conv2d_147, False\n",
      "Layer: conv2d_152, False\n",
      "Layer: conv2d_153, False\n",
      "Layer: batch_normalization_149, False\n",
      "Layer: batch_normalization_152, False\n",
      "Layer: batch_normalization_157, False\n",
      "Layer: batch_normalization_158, False\n",
      "Layer: activation_148, False\n",
      "Layer: activation_151, False\n",
      "Layer: activation_156, False\n",
      "Layer: activation_157, False\n",
      "Layer: mixed6, False\n",
      "Layer: conv2d_158, False\n",
      "Layer: batch_normalization_163, False\n",
      "Layer: activation_162, False\n",
      "Layer: conv2d_159, False\n",
      "Layer: batch_normalization_164, False\n",
      "Layer: activation_163, False\n",
      "Layer: conv2d_155, False\n",
      "Layer: conv2d_160, False\n",
      "Layer: batch_normalization_160, False\n",
      "Layer: batch_normalization_165, False\n",
      "Layer: activation_159, False\n",
      "Layer: activation_164, False\n",
      "Layer: conv2d_156, False\n",
      "Layer: conv2d_161, False\n",
      "Layer: batch_normalization_161, False\n",
      "Layer: batch_normalization_166, False\n",
      "Layer: activation_160, False\n",
      "Layer: activation_165, False\n",
      "Layer: average_pooling2d_15, False\n",
      "Layer: conv2d_154, False\n",
      "Layer: conv2d_157, False\n",
      "Layer: conv2d_162, False\n",
      "Layer: conv2d_163, False\n",
      "Layer: batch_normalization_159, False\n",
      "Layer: batch_normalization_162, False\n",
      "Layer: batch_normalization_167, False\n",
      "Layer: batch_normalization_168, False\n",
      "Layer: activation_158, False\n",
      "Layer: activation_161, False\n",
      "Layer: activation_166, False\n",
      "Layer: activation_167, False\n",
      "Layer: mixed7, False\n",
      "Layer: conv2d_166, False\n",
      "Layer: batch_normalization_171, False\n",
      "Layer: activation_170, False\n",
      "Layer: conv2d_167, False\n",
      "Layer: batch_normalization_172, False\n",
      "Layer: activation_171, False\n",
      "Layer: conv2d_164, False\n",
      "Layer: conv2d_168, False\n",
      "Layer: batch_normalization_169, False\n",
      "Layer: batch_normalization_173, False\n",
      "Layer: activation_168, False\n",
      "Layer: activation_172, False\n",
      "Layer: conv2d_165, False\n",
      "Layer: conv2d_169, False\n",
      "Layer: batch_normalization_170, False\n",
      "Layer: batch_normalization_174, False\n",
      "Layer: activation_169, False\n",
      "Layer: activation_173, False\n",
      "Layer: max_pooling2d_7, False\n",
      "Layer: mixed8, False\n",
      "Layer: conv2d_174, False\n",
      "Layer: batch_normalization_179, False\n",
      "Layer: activation_178, False\n",
      "Layer: conv2d_171, False\n",
      "Layer: conv2d_175, False\n",
      "Layer: batch_normalization_176, False\n",
      "Layer: batch_normalization_180, False\n",
      "Layer: activation_175, False\n",
      "Layer: activation_179, False\n",
      "Layer: conv2d_172, False\n",
      "Layer: conv2d_173, False\n",
      "Layer: conv2d_176, False\n",
      "Layer: conv2d_177, False\n",
      "Layer: average_pooling2d_16, False\n",
      "Layer: conv2d_170, False\n",
      "Layer: batch_normalization_177, False\n",
      "Layer: batch_normalization_178, False\n",
      "Layer: batch_normalization_181, False\n",
      "Layer: batch_normalization_182, False\n",
      "Layer: conv2d_178, False\n",
      "Layer: batch_normalization_175, False\n",
      "Layer: activation_176, False\n",
      "Layer: activation_177, False\n",
      "Layer: activation_180, False\n",
      "Layer: activation_181, False\n",
      "Layer: batch_normalization_183, False\n",
      "Layer: activation_174, False\n",
      "Layer: mixed9_0, False\n",
      "Layer: concatenate_2, False\n",
      "Layer: activation_182, False\n",
      "Layer: mixed9, False\n",
      "Layer: conv2d_183, False\n",
      "Layer: batch_normalization_188, False\n",
      "Layer: activation_187, False\n",
      "Layer: conv2d_180, False\n",
      "Layer: conv2d_184, False\n",
      "Layer: batch_normalization_185, False\n",
      "Layer: batch_normalization_189, False\n",
      "Layer: activation_184, False\n",
      "Layer: activation_188, False\n",
      "Layer: conv2d_181, False\n",
      "Layer: conv2d_182, False\n",
      "Layer: conv2d_185, False\n",
      "Layer: conv2d_186, False\n",
      "Layer: average_pooling2d_17, False\n",
      "Layer: conv2d_179, False\n",
      "Layer: batch_normalization_186, False\n",
      "Layer: batch_normalization_187, False\n",
      "Layer: batch_normalization_190, False\n",
      "Layer: batch_normalization_191, False\n",
      "Layer: conv2d_187, False\n",
      "Layer: batch_normalization_184, False\n",
      "Layer: activation_185, False\n",
      "Layer: activation_186, False\n",
      "Layer: activation_189, False\n",
      "Layer: activation_190, False\n",
      "Layer: batch_normalization_192, False\n",
      "Layer: activation_183, False\n",
      "Layer: mixed9_1, False\n",
      "Layer: concatenate_3, False\n",
      "Layer: activation_191, False\n",
      "Layer: mixed10, False\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:54.550562Z",
     "start_time": "2024-07-29T20:19:54.543621Z"
    }
   },
   "cell_type": "code",
   "source": "len(base_model.layers)",
   "id": "4d5901824515235e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:54.554660Z",
     "start_time": "2024-07-29T20:19:54.551457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "No_of_classes = len(os.listdir(fundus_train))\n",
    "No_of_classes"
   ],
   "id": "c24aa2a52df99eca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:54.559920Z",
     "start_time": "2024-07-29T20:19:54.555158Z"
    }
   },
   "cell_type": "code",
   "source": "aug_layer = utils.return_data_aug_layer_for_eff_net()",
   "id": "e1fbc17ac5520d23",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:55.454224Z",
     "start_time": "2024-07-29T20:19:55.135573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, GlobalAvgPool2D, Input, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "inputs = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3), name = 'Input_layer')\n",
    "#x = aug_layer(inputs)\n",
    "x = tf.keras.layers.Rescaling(1./255)(inputs)\n",
    "x = base_model(x, training = False)\n",
    "x = layers.GlobalAvgPool2D()(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Dense(512, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "x = Dense(256, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "\n",
    "x = Dense(64, kernel_regularizer=l2(0.01))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tf.keras.layers.Activation('relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "Outputs = Dense(No_of_classes, activation = 'softmax', dtype = tf.float32)(x)\n",
    "\n",
    "model_1 = Model(inputs, Outputs, name = 'Inceptionv')\n",
    "\n",
    "model_1.summary()"
   ],
   "id": "f8314db62f9c88ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Inceptionv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_193 (B  (None, 2048)              8192      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_194 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_192 (Activation  (None, 512)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_195 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_193 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_196 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_194 (Activation  (None, 128)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_197 (B  (None, 64)                256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_195 (Activation  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23036644 (87.88 MB)\n",
      "Trainable params: 1227844 (4.68 MB)\n",
      "Non-trainable params: 21808800 (83.19 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:56.258248Z",
     "start_time": "2024-07-29T20:19:56.255836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Model checkpointing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "model_1chkpt = ModelCheckpoint(filepath = os.path.join('Trained_Models',model_1.name), save_weights_only = False, save_best_only = True, verbose = 1)"
   ],
   "id": "a30cb8302eb2cd34",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:56.500761Z",
     "start_time": "2024-07-29T20:19:56.498521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.src.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(factor=0.3, patience=3) "
   ],
   "id": "48463ce3a6a20776",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:19:56.721557Z",
     "start_time": "2024-07-29T20:19:56.713641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#model compilation\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.001), metrics = ['accuracy'])"
   ],
   "id": "18264d9b28fa328f",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:23:06.531917Z",
     "start_time": "2024-07-29T20:19:57.403400Z"
    }
   },
   "cell_type": "code",
   "source": "history_1 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = 15, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "85e1fa6dc6835069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "73/74 [============================>.] - ETA: 0s - loss: 11.6755 - accuracy: 0.5428\n",
      "Epoch 1: val_loss improved from inf to 8.28978, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 18s 208ms/step - loss: 11.6386 - accuracy: 0.5426 - val_loss: 8.2898 - val_accuracy: 0.8487 - lr: 0.0010\n",
      "Epoch 2/15\n",
      "74/74 [==============================] - ETA: 0s - loss: 6.2162 - accuracy: 0.6823\n",
      "Epoch 2: val_loss improved from 8.28978 to 4.69458, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 210ms/step - loss: 6.2162 - accuracy: 0.6823 - val_loss: 4.6946 - val_accuracy: 0.8487 - lr: 0.0010\n",
      "Epoch 3/15\n",
      "72/74 [============================>.] - ETA: 0s - loss: 3.8516 - accuracy: 0.7140\n",
      "Epoch 3: val_loss improved from 4.69458 to 3.07008, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 207ms/step - loss: 3.8341 - accuracy: 0.7141 - val_loss: 3.0701 - val_accuracy: 0.8487 - lr: 0.0010\n",
      "Epoch 4/15\n",
      "72/74 [============================>.] - ETA: 0s - loss: 2.7064 - accuracy: 0.7418\n",
      "Epoch 4: val_loss improved from 3.07008 to 2.21337, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 208ms/step - loss: 2.6969 - accuracy: 0.7425 - val_loss: 2.2134 - val_accuracy: 0.8506 - lr: 0.0010\n",
      "Epoch 5/15\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.1178 - accuracy: 0.7370\n",
      "Epoch 5: val_loss improved from 2.21337 to 1.73285, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 15s 207ms/step - loss: 2.1178 - accuracy: 0.7370 - val_loss: 1.7328 - val_accuracy: 0.9080 - lr: 0.0010\n",
      "Epoch 6/15\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.7359 - accuracy: 0.7551\n",
      "Epoch 6: val_loss improved from 1.73285 to 1.51305, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 208ms/step - loss: 1.7373 - accuracy: 0.7548 - val_loss: 1.5130 - val_accuracy: 0.9219 - lr: 0.0010\n",
      "Epoch 7/15\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.4956 - accuracy: 0.7449\n",
      "Epoch 7: val_loss improved from 1.51305 to 1.33946, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 15s 207ms/step - loss: 1.4961 - accuracy: 0.7442 - val_loss: 1.3395 - val_accuracy: 0.9139 - lr: 0.0010\n",
      "Epoch 8/15\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.3677 - accuracy: 0.7461\n",
      "Epoch 8: val_loss improved from 1.33946 to 1.32115, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 208ms/step - loss: 1.3692 - accuracy: 0.7459 - val_loss: 1.3212 - val_accuracy: 0.8140 - lr: 0.0010\n",
      "Epoch 9/15\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.2228 - accuracy: 0.7680\n",
      "Epoch 9: val_loss improved from 1.32115 to 1.24397, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 209ms/step - loss: 1.2231 - accuracy: 0.7675 - val_loss: 1.2440 - val_accuracy: 0.6934 - lr: 0.0010\n",
      "Epoch 10/15\n",
      "72/74 [============================>.] - ETA: 0s - loss: 1.0978 - accuracy: 0.7752\n",
      "Epoch 10: val_loss improved from 1.24397 to 1.09190, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 208ms/step - loss: 1.0957 - accuracy: 0.7755 - val_loss: 1.0919 - val_accuracy: 0.8061 - lr: 0.0010\n",
      "Epoch 11/15\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1520 - accuracy: 0.7429\n",
      "Epoch 11: val_loss did not improve from 1.09190\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 1.1520 - accuracy: 0.7429 - val_loss: 1.2441 - val_accuracy: 0.7389 - lr: 0.0010\n",
      "Epoch 12/15\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.9903 - accuracy: 0.7847\n",
      "Epoch 12: val_loss improved from 1.09190 to 0.95585, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 216ms/step - loss: 0.9889 - accuracy: 0.7840 - val_loss: 0.9559 - val_accuracy: 0.8101 - lr: 0.0010\n",
      "Epoch 13/15\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.0084 - accuracy: 0.7586\n",
      "Epoch 13: val_loss did not improve from 0.95585\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 1.0084 - accuracy: 0.7586 - val_loss: 1.2210 - val_accuracy: 0.6607 - lr: 0.0010\n",
      "Epoch 14/15\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.9946 - accuracy: 0.7700\n",
      "Epoch 14: val_loss did not improve from 0.95585\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.9907 - accuracy: 0.7713 - val_loss: 1.0927 - val_accuracy: 0.7319 - lr: 0.0010\n",
      "Epoch 15/15\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.9273 - accuracy: 0.7873\n",
      "Epoch 15: val_loss did not improve from 0.95585\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.9256 - accuracy: 0.7874 - val_loss: 1.0134 - val_accuracy: 0.7478 - lr: 0.0010\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:25:50.929986Z",
     "start_time": "2024-07-29T20:23:18.738458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 15\n",
    "history_1 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+15,initial_epoch=start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))"
   ],
   "id": "6198b63c23a604a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7897 - accuracy: 0.8247\n",
      "Epoch 16: val_loss improved from 0.95585 to 0.65949, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 212ms/step - loss: 0.7897 - accuracy: 0.8247 - val_loss: 0.6595 - val_accuracy: 0.9189 - lr: 3.0000e-04\n",
      "Epoch 17/30\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.7330 - accuracy: 0.8255\n",
      "Epoch 17: val_loss did not improve from 0.65949\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.7351 - accuracy: 0.8263 - val_loss: 0.6689 - val_accuracy: 0.9060 - lr: 3.0000e-04\n",
      "Epoch 18/30\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.7068 - accuracy: 0.8303\n",
      "Epoch 18: val_loss improved from 0.65949 to 0.63536, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 208ms/step - loss: 0.7042 - accuracy: 0.8310 - val_loss: 0.6354 - val_accuracy: 0.8932 - lr: 3.0000e-04\n",
      "Epoch 19/30\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.6519 - accuracy: 0.8451\n",
      "Epoch 19: val_loss did not improve from 0.63536\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.6472 - accuracy: 0.8471 - val_loss: 0.7108 - val_accuracy: 0.8131 - lr: 3.0000e-04\n",
      "Epoch 20/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6191 - accuracy: 0.8526\n",
      "Epoch 20: val_loss improved from 0.63536 to 0.62936, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 221ms/step - loss: 0.6191 - accuracy: 0.8526 - val_loss: 0.6294 - val_accuracy: 0.8655 - lr: 3.0000e-04\n",
      "Epoch 21/30\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6231 - accuracy: 0.8493\n",
      "Epoch 21: val_loss improved from 0.62936 to 0.61266, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 219ms/step - loss: 0.6223 - accuracy: 0.8488 - val_loss: 0.6127 - val_accuracy: 0.8615 - lr: 3.0000e-04\n",
      "Epoch 22/30\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6493 - accuracy: 0.8373\n",
      "Epoch 22: val_loss improved from 0.61266 to 0.56554, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 218ms/step - loss: 0.6528 - accuracy: 0.8357 - val_loss: 0.5655 - val_accuracy: 0.9031 - lr: 3.0000e-04\n",
      "Epoch 23/30\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6078 - accuracy: 0.8545\n",
      "Epoch 23: val_loss did not improve from 0.56554\n",
      "74/74 [==============================] - 4s 43ms/step - loss: 0.6078 - accuracy: 0.8539 - val_loss: 0.6135 - val_accuracy: 0.9031 - lr: 3.0000e-04\n",
      "Epoch 24/30\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.5673 - accuracy: 0.8604\n",
      "Epoch 24: val_loss did not improve from 0.56554\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 0.5687 - accuracy: 0.8594 - val_loss: 0.5832 - val_accuracy: 0.8872 - lr: 3.0000e-04\n",
      "Epoch 25/30\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.5854 - accuracy: 0.8476\n",
      "Epoch 25: val_loss did not improve from 0.56554\n",
      "74/74 [==============================] - 4s 51ms/step - loss: 0.5840 - accuracy: 0.8484 - val_loss: 0.5832 - val_accuracy: 0.8744 - lr: 3.0000e-04\n",
      "Epoch 26/30\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.5310 - accuracy: 0.8716\n",
      "Epoch 26: val_loss improved from 0.56554 to 0.50557, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 208ms/step - loss: 0.5317 - accuracy: 0.8717 - val_loss: 0.5056 - val_accuracy: 0.9060 - lr: 9.0000e-05\n",
      "Epoch 27/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4922 - accuracy: 0.8873\n",
      "Epoch 27: val_loss improved from 0.50557 to 0.49455, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 15s 204ms/step - loss: 0.4922 - accuracy: 0.8873 - val_loss: 0.4946 - val_accuracy: 0.9011 - lr: 9.0000e-05\n",
      "Epoch 28/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.8954\n",
      "Epoch 28: val_loss improved from 0.49455 to 0.44336, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 15s 205ms/step - loss: 0.4640 - accuracy: 0.8954 - val_loss: 0.4434 - val_accuracy: 0.9219 - lr: 9.0000e-05\n",
      "Epoch 29/30\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.4632 - accuracy: 0.8893\n",
      "Epoch 29: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 4s 43ms/step - loss: 0.4628 - accuracy: 0.8890 - val_loss: 0.4595 - val_accuracy: 0.9149 - lr: 9.0000e-05\n",
      "Epoch 30/30\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4635 - accuracy: 0.8886\n",
      "Epoch 30: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.4635 - accuracy: 0.8886 - val_loss: 0.5222 - val_accuracy: 0.8952 - lr: 9.0000e-05\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:26:56.356777Z",
     "start_time": "2024-07-29T20:26:54.909711Z"
    }
   },
   "cell_type": "code",
   "source": "model_1.evaluate(test_datagen)",
   "id": "eb9cc0053cd28662",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 51ms/step - loss: 0.5858 - accuracy: 0.8355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5858327150344849, 0.8355029821395874]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:26:59.980560Z",
     "start_time": "2024-07-29T20:26:59.974980Z"
    }
   },
   "cell_type": "code",
   "source": "base_model.trainable = True",
   "id": "9b7e0fe022b2e5ad",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:27:00.346814Z",
     "start_time": "2024-07-29T20:27:00.341620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers[:-25]:\n",
    "    layer.trainable = False"
   ],
   "id": "144c133d132993be",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:27:01.958464Z",
     "start_time": "2024-07-29T20:27:01.955152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for layer in base_model.layers:\n",
    "    print(layer.trainable)"
   ],
   "id": "a853d58f3af9fa71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:27:17.125528Z",
     "start_time": "2024-07-29T20:27:17.085865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 30\n",
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0003), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "7233597bf79998e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Inceptionv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_193 (B  (None, 2048)              8192      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_194 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_192 (Activation  (None, 512)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_195 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_193 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_196 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_194 (Activation  (None, 128)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_197 (B  (None, 64)                256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_195 (Activation  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23036644 (87.88 MB)\n",
      "Trainable params: 4048324 (15.44 MB)\n",
      "Non-trainable params: 18988320 (72.43 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:28:25.839392Z",
     "start_time": "2024-07-29T20:27:21.647150Z"
    }
   },
   "cell_type": "code",
   "source": "history_50 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "eeb24a49a8759ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.8658 - accuracy: 0.7281\n",
      "Epoch 31: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 8s 44ms/step - loss: 0.8658 - accuracy: 0.7281 - val_loss: 4.9273 - val_accuracy: 0.1246 - lr: 3.0000e-04\n",
      "Epoch 32/40\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6943 - accuracy: 0.7980\n",
      "Epoch 32: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.6943 - accuracy: 0.7980 - val_loss: 0.5453 - val_accuracy: 0.8576 - lr: 3.0000e-04\n",
      "Epoch 33/40\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.6229 - accuracy: 0.8203\n",
      "Epoch 33: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.6252 - accuracy: 0.8200 - val_loss: 1.4857 - val_accuracy: 0.2285 - lr: 3.0000e-04\n",
      "Epoch 34/40\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.6058 - accuracy: 0.8234\n",
      "Epoch 34: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 3s 42ms/step - loss: 0.6071 - accuracy: 0.8230 - val_loss: 0.9559 - val_accuracy: 0.6845 - lr: 3.0000e-04\n",
      "Epoch 35/40\n",
      "72/74 [============================>.] - ETA: 0s - loss: 0.5539 - accuracy: 0.8416\n",
      "Epoch 35: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.5534 - accuracy: 0.8420 - val_loss: 1.0449 - val_accuracy: 0.6647 - lr: 3.0000e-04\n",
      "Epoch 36/40\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4789 - accuracy: 0.8750\n",
      "Epoch 36: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 3s 43ms/step - loss: 0.4782 - accuracy: 0.8759 - val_loss: 0.5149 - val_accuracy: 0.8991 - lr: 9.0000e-05\n",
      "Epoch 37/40\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4497 - accuracy: 0.8797\n",
      "Epoch 37: val_loss did not improve from 0.44336\n",
      "74/74 [==============================] - 4s 45ms/step - loss: 0.4497 - accuracy: 0.8797 - val_loss: 0.6134 - val_accuracy: 0.8398 - lr: 9.0000e-05\n",
      "Epoch 38/40\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4118 - accuracy: 0.8992\n",
      "Epoch 38: val_loss improved from 0.44336 to 0.36625, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 212ms/step - loss: 0.4118 - accuracy: 0.8992 - val_loss: 0.3662 - val_accuracy: 0.9070 - lr: 9.0000e-05\n",
      "Epoch 39/40\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.8801\n",
      "Epoch 39: val_loss improved from 0.36625 to 0.35169, saving model to Trained_Models/Inceptionv\n",
      "74/74 [==============================] - 16s 212ms/step - loss: 0.4321 - accuracy: 0.8801 - val_loss: 0.3517 - val_accuracy: 0.9159 - lr: 9.0000e-05\n",
      "Epoch 40/40\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8930\n",
      "Epoch 40: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.4108 - accuracy: 0.8928 - val_loss: 0.5625 - val_accuracy: 0.8665 - lr: 9.0000e-05\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:29:33.941994Z",
     "start_time": "2024-07-29T20:29:33.215075Z"
    }
   },
   "cell_type": "code",
   "source": "model_1.evaluate(test_datagen)",
   "id": "76e9cd11a6024452",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 24ms/step - loss: 0.6225 - accuracy: 0.8201\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6224556565284729, 0.8201183676719666]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:29:39.879012Z",
     "start_time": "2024-07-29T20:29:39.875903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 40\n",
    "for layer in base_model.layers[-75:]:\n",
    "    layer.trainable = True"
   ],
   "id": "6a7cd8da92ae1b49",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:29:40.982541Z",
     "start_time": "2024-07-29T20:29:40.953405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.0003), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "1e494e7e29768448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Inceptionv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_193 (B  (None, 2048)              8192      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_194 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_192 (Activation  (None, 512)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_195 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_193 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_196 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_194 (Activation  (None, 128)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_197 (B  (None, 64)                256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_195 (Activation  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23036644 (87.88 MB)\n",
      "Trainable params: 13486404 (51.45 MB)\n",
      "Non-trainable params: 9550240 (36.43 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:31:17.061195Z",
     "start_time": "2024-07-29T20:30:38.951838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 50\n",
    "history_100 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))"
   ],
   "id": "81d7f639996a2cff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/60\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4268 - accuracy: 0.8750\n",
      "Epoch 51: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 47ms/step - loss: 0.4266 - accuracy: 0.8755 - val_loss: 7.3225 - val_accuracy: 0.0425 - lr: 9.0000e-05\n",
      "Epoch 52/60\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4139 - accuracy: 0.8734\n",
      "Epoch 52: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 43ms/step - loss: 0.4139 - accuracy: 0.8734 - val_loss: 16.1631 - val_accuracy: 0.0000e+00 - lr: 9.0000e-05\n",
      "Epoch 53/60\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4175 - accuracy: 0.8776\n",
      "Epoch 53: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 43ms/step - loss: 0.4175 - accuracy: 0.8776 - val_loss: 1.5933 - val_accuracy: 0.6222 - lr: 9.0000e-05\n",
      "Epoch 54/60\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.3940 - accuracy: 0.8793\n",
      "Epoch 54: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 44ms/step - loss: 0.3940 - accuracy: 0.8793 - val_loss: 3.4947 - val_accuracy: 0.2591 - lr: 9.0000e-05\n",
      "Epoch 55/60\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.3854 - accuracy: 0.8780\n",
      "Epoch 55: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 44ms/step - loss: 0.3854 - accuracy: 0.8780 - val_loss: 0.5134 - val_accuracy: 0.8417 - lr: 9.0000e-05\n",
      "Epoch 56/60\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4053 - accuracy: 0.8694\n",
      "Epoch 56: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 45ms/step - loss: 0.4054 - accuracy: 0.8691 - val_loss: 36.4777 - val_accuracy: 0.0000e+00 - lr: 9.0000e-05\n",
      "Epoch 57/60\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4129 - accuracy: 0.8707\n",
      "Epoch 57: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 45ms/step - loss: 0.4123 - accuracy: 0.8712 - val_loss: 15.4102 - val_accuracy: 0.0000e+00 - lr: 9.0000e-05\n",
      "Epoch 58/60\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4040 - accuracy: 0.8810\n",
      "Epoch 58: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 52ms/step - loss: 0.4030 - accuracy: 0.8810 - val_loss: 6.3440 - val_accuracy: 0.0267 - lr: 9.0000e-05\n",
      "Epoch 59/60\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3488 - accuracy: 0.8981\n",
      "Epoch 59: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.3486 - accuracy: 0.8988 - val_loss: 4.8537 - val_accuracy: 0.1632 - lr: 2.7000e-05\n",
      "Epoch 60/60\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.3322 - accuracy: 0.8990\n",
      "Epoch 60: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 54ms/step - loss: 0.3329 - accuracy: 0.8983 - val_loss: 0.7068 - val_accuracy: 0.7626 - lr: 2.7000e-05\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:31:58.879904Z",
     "start_time": "2024-07-29T20:31:58.110370Z"
    }
   },
   "cell_type": "code",
   "source": "model_1.evaluate(test_datagen)",
   "id": "cb9409c4dba63f27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 26ms/step - loss: 0.9410 - accuracy: 0.7290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9409737586975098, 0.7289940714836121]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:32:17.971775Z",
     "start_time": "2024-07-29T20:32:17.968135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 60\n",
    "\n",
    "for layer in base_model.layers[-125:]:\n",
    "    layer.trainable = True"
   ],
   "id": "9c3fa202d236af9e",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:32:19.775077Z",
     "start_time": "2024-07-29T20:32:19.733374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.000075), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "10b77e72c3b9b02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Inceptionv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " rescaling_1 (Rescaling)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " inception_v3 (Functional)   (None, 5, 5, 2048)        21802784  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " batch_normalization_193 (B  (None, 2048)              8192      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 512)               1049088   \n",
      "                                                                 \n",
      " batch_normalization_194 (B  (None, 512)               2048      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_192 (Activation  (None, 512)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_195 (B  (None, 256)               1024      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_193 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_196 (B  (None, 128)               512       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_194 (Activation  (None, 128)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_197 (B  (None, 64)                256       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " activation_195 (Activation  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23036644 (87.88 MB)\n",
      "Trainable params: 16543044 (63.11 MB)\n",
      "Non-trainable params: 6493600 (24.77 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T20:33:05.062544Z",
     "start_time": "2024-07-29T20:32:22.416081Z"
    }
   },
   "cell_type": "code",
   "source": "history_200 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "38d94ee2c181b1df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/70\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.5859 - accuracy: 0.8094\n",
      "Epoch 61: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 12s 57ms/step - loss: 0.5859 - accuracy: 0.8094 - val_loss: 48.6632 - val_accuracy: 0.0000e+00 - lr: 7.5000e-05\n",
      "Epoch 62/70\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4867 - accuracy: 0.8234\n",
      "Epoch 62: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.4867 - accuracy: 0.8234 - val_loss: 34.9214 - val_accuracy: 0.0000e+00 - lr: 7.5000e-05\n",
      "Epoch 63/70\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4958 - accuracy: 0.8309\n",
      "Epoch 63: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 0.4962 - accuracy: 0.8306 - val_loss: 102.3690 - val_accuracy: 0.0000e+00 - lr: 7.5000e-05\n",
      "Epoch 64/70\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.8217\n",
      "Epoch 64: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.5062 - accuracy: 0.8217 - val_loss: 36.7897 - val_accuracy: 0.0000e+00 - lr: 7.5000e-05\n",
      "Epoch 65/70\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4794 - accuracy: 0.8331\n",
      "Epoch 65: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.4794 - accuracy: 0.8331 - val_loss: 22.1745 - val_accuracy: 0.0000e+00 - lr: 7.5000e-05\n",
      "Epoch 66/70\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4982 - accuracy: 0.8296\n",
      "Epoch 66: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 48ms/step - loss: 0.4971 - accuracy: 0.8302 - val_loss: 27.9281 - val_accuracy: 0.0000e+00 - lr: 7.5000e-05\n",
      "Epoch 67/70\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.4369 - accuracy: 0.8523\n",
      "Epoch 67: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 49ms/step - loss: 0.4402 - accuracy: 0.8509 - val_loss: 14.6056 - val_accuracy: 0.0020 - lr: 7.5000e-05\n",
      "Epoch 68/70\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.8441\n",
      "Epoch 68: val_loss did not improve from 0.35169\n",
      "74/74 [==============================] - 4s 56ms/step - loss: 0.4574 - accuracy: 0.8441 - val_loss: 34.1168 - val_accuracy: 0.1503 - lr: 7.5000e-05\n",
      "Epoch 69/70\n",
      "56/74 [=====================>........] - ETA: 0s - loss: 0.4355 - accuracy: 0.8499"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m history_200 \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_datagen\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mval_datagen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mstart_epoch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodel_1chkpt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_datagen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_steps\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mval_datagen\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1775\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1776\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1777\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1780\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1781\u001B[0m ):\n\u001B[1;32m   1782\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1783\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1785\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    828\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    830\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 831\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    833\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    834\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    864\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    865\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    866\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 867\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    868\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_config\u001B[49m\n\u001B[1;32m    869\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    870\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    871\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    872\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    873\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001B[0m, in \u001B[0;36mcall_function\u001B[0;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[1;32m    137\u001B[0m bound_args \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mbind(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    138\u001B[0m flat_inputs \u001B[38;5;241m=\u001B[39m function\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39munpack_inputs(bound_args)\n\u001B[0;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=protected-access\u001B[39;49;00m\n\u001B[1;32m    140\u001B[0m \u001B[43m    \u001B[49m\u001B[43mflat_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, tensor_inputs, captured_inputs)\u001B[0m\n\u001B[1;32m   1260\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1261\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1262\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1263\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1264\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1265\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1266\u001B[0m     args,\n\u001B[1;32m   1267\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1268\u001B[0m     executing_eagerly)\n\u001B[1;32m   1269\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001B[0m, in \u001B[0;36mAtomicFunction.flat_call\u001B[0;34m(self, args)\u001B[0m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mflat_call\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: Sequence[core\u001B[38;5;241m.\u001B[39mTensor]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[1;32m    216\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 217\u001B[0m   flat_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    218\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunction_type\u001B[38;5;241m.\u001B[39mpack_output(flat_outputs)\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001B[0m, in \u001B[0;36mAtomicFunction.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m record\u001B[38;5;241m.\u001B[39mstop_recording():\n\u001B[1;32m    251\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 252\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_bound_context\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunction_type\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_outputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    257\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    258\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m make_call_op_in_graph(\n\u001B[1;32m    259\u001B[0m         \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28mlist\u001B[39m(args),\n\u001B[1;32m    261\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_bound_context\u001B[38;5;241m.\u001B[39mfunction_call_options\u001B[38;5;241m.\u001B[39mas_attrs(),\n\u001B[1;32m    262\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1479\u001B[0m, in \u001B[0;36mContext.call_function\u001B[0;34m(self, name, tensor_inputs, num_outputs)\u001B[0m\n\u001B[1;32m   1477\u001B[0m cancellation_context \u001B[38;5;241m=\u001B[39m cancellation\u001B[38;5;241m.\u001B[39mcontext()\n\u001B[1;32m   1478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cancellation_context \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1479\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1480\u001B[0m \u001B[43m      \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1481\u001B[0m \u001B[43m      \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1482\u001B[0m \u001B[43m      \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtensor_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1483\u001B[0m \u001B[43m      \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1484\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1485\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1486\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1487\u001B[0m   outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m   1488\u001B[0m       name\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[1;32m   1489\u001B[0m       num_outputs\u001B[38;5;241m=\u001B[39mnum_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1493\u001B[0m       cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_context,\n\u001B[1;32m   1494\u001B[0m   )\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:60\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     53\u001B[0m   \u001B[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001B[39;00m\n\u001B[1;32m     54\u001B[0m   inputs \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     55\u001B[0m       tensor_conversion_registry\u001B[38;5;241m.\u001B[39mconvert(t)\n\u001B[1;32m     56\u001B[0m       \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(t, core_types\u001B[38;5;241m.\u001B[39mTensor)\n\u001B[1;32m     57\u001B[0m       \u001B[38;5;28;01melse\u001B[39;00m t\n\u001B[1;32m     58\u001B[0m       \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m inputs\n\u001B[1;32m     59\u001B[0m   ]\n\u001B[0;32m---> 60\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     63\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:47:35.569625Z",
     "start_time": "2024-07-29T19:47:34.535022Z"
    }
   },
   "cell_type": "code",
   "source": "model_1.evaluate(test_datagen)",
   "id": "572beefd899e12a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 35ms/step - loss: 0.4816 - accuracy: 0.8828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.48157748579978943, 0.8828402161598206]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:47:38.807369Z",
     "start_time": "2024-07-29T19:47:38.801646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 45\n",
    "for layer in base_model.layers[-300:]:\n",
    "    layer.trainable = True"
   ],
   "id": "93ad3cc030c9c69",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:47:39.712985Z",
     "start_time": "2024-07-29T19:47:39.665045Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.000025), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "9ed4bf4a63f90d77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Nasnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " NASNet (Functional)         (None, 7, 7, 1056)        4269716   \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1056)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1056)              4224      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               541184    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_188 (Activation  (None, 512)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_189 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_190 (Activation  (None, 128)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_191 (Activation  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4991704 (19.04 MB)\n",
      "Trainable params: 4121356 (15.72 MB)\n",
      "Non-trainable params: 870348 (3.32 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:49:16.600177Z",
     "start_time": "2024-07-29T19:47:41.082633Z"
    }
   },
   "cell_type": "code",
   "source": "history_500 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "b0794490c20e0209",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1190 - accuracy: 0.9856\n",
      "Epoch 46: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 36s 107ms/step - loss: 0.1190 - accuracy: 0.9856 - val_loss: 0.2369 - val_accuracy: 0.9585 - lr: 2.5000e-05\n",
      "Epoch 47/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1114 - accuracy: 0.9894\n",
      "Epoch 47: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.1114 - accuracy: 0.9894 - val_loss: 0.4673 - val_accuracy: 0.8724 - lr: 2.5000e-05\n",
      "Epoch 48/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1195 - accuracy: 0.9886\n",
      "Epoch 48: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.1195 - accuracy: 0.9886 - val_loss: 0.2321 - val_accuracy: 0.9525 - lr: 2.5000e-05\n",
      "Epoch 49/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0976 - accuracy: 0.9936\n",
      "Epoch 49: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.0976 - accuracy: 0.9936 - val_loss: 0.1823 - val_accuracy: 0.9703 - lr: 2.5000e-05\n",
      "Epoch 50/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0975 - accuracy: 0.9941\n",
      "Epoch 50: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 7s 84ms/step - loss: 0.0975 - accuracy: 0.9941 - val_loss: 0.1955 - val_accuracy: 0.9644 - lr: 2.5000e-05\n",
      "Epoch 51/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1015 - accuracy: 0.9915\n",
      "Epoch 51: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 7s 84ms/step - loss: 0.1015 - accuracy: 0.9915 - val_loss: 0.2076 - val_accuracy: 0.9624 - lr: 2.5000e-05\n",
      "Epoch 52/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9924\n",
      "Epoch 52: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 7s 91ms/step - loss: 0.0962 - accuracy: 0.9924 - val_loss: 0.1759 - val_accuracy: 0.9733 - lr: 2.5000e-05\n",
      "Epoch 53/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9920\n",
      "Epoch 53: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 7s 88ms/step - loss: 0.1000 - accuracy: 0.9920 - val_loss: 0.2659 - val_accuracy: 0.9505 - lr: 2.5000e-05\n",
      "Epoch 54/55\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0947 - accuracy: 0.9915\n",
      "Epoch 54: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 7s 89ms/step - loss: 0.0947 - accuracy: 0.9915 - val_loss: 0.2483 - val_accuracy: 0.9525 - lr: 2.5000e-05\n",
      "Epoch 55/55\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.0839 - accuracy: 0.9957\n",
      "Epoch 55: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 6s 83ms/step - loss: 0.0838 - accuracy: 0.9958 - val_loss: 0.2698 - val_accuracy: 0.9496 - lr: 2.5000e-05\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:49:44.279319Z",
     "start_time": "2024-07-29T19:49:44.269349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_epoch = 55\n",
    "for layer in base_model.layers[-700:]:\n",
    "    layer.trainable = True"
   ],
   "id": "65b379ec652de956",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:49:45.195948Z",
     "start_time": "2024-07-29T19:49:45.117047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1.compile(loss = 'categorical_crossentropy', optimizer = Adam(learning_rate = 0.000025), metrics = ['accuracy'])\n",
    "model_1.summary()"
   ],
   "id": "c49c24beb9980801",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Nasnet\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " NASNet (Functional)         (None, 7, 7, 1056)        4269716   \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 1056)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 1056)              4224      \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               541184    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 512)               2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_188 (Activation  (None, 512)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_189 (Activation  (None, 256)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_190 (Activation  (None, 128)               0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_191 (Activation  (None, 64)                0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4991704 (19.04 MB)\n",
      "Trainable params: 4937116 (18.83 MB)\n",
      "Non-trainable params: 54588 (213.23 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:51:58.666869Z",
     "start_time": "2024-07-29T19:49:47.229317Z"
    }
   },
   "cell_type": "code",
   "source": "history_500 = model_1.fit(train_datagen, validation_data = (val_datagen), epochs = start_epoch+10, initial_epoch = start_epoch, verbose = 1, callbacks = [model_1chkpt, lr_scheduler], steps_per_epoch = len(train_datagen), validation_steps = len(val_datagen))",
   "id": "5fd98f0c46842888",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9814\n",
      "Epoch 56: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 55s 148ms/step - loss: 0.1236 - accuracy: 0.9814 - val_loss: 0.3335 - val_accuracy: 0.9308 - lr: 2.5000e-05\n",
      "Epoch 57/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1372 - accuracy: 0.9809\n",
      "Epoch 57: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 8s 107ms/step - loss: 0.1372 - accuracy: 0.9809 - val_loss: 0.1625 - val_accuracy: 0.9763 - lr: 2.5000e-05\n",
      "Epoch 58/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9903\n",
      "Epoch 58: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 8s 108ms/step - loss: 0.1013 - accuracy: 0.9903 - val_loss: 0.2233 - val_accuracy: 0.9594 - lr: 2.5000e-05\n",
      "Epoch 59/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9911\n",
      "Epoch 59: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 8s 108ms/step - loss: 0.0955 - accuracy: 0.9911 - val_loss: 0.2628 - val_accuracy: 0.9476 - lr: 2.5000e-05\n",
      "Epoch 60/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0850 - accuracy: 0.9932\n",
      "Epoch 60: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.0850 - accuracy: 0.9932 - val_loss: 0.2176 - val_accuracy: 0.9585 - lr: 2.5000e-05\n",
      "Epoch 61/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9953\n",
      "Epoch 61: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 9s 111ms/step - loss: 0.0802 - accuracy: 0.9953 - val_loss: 0.2081 - val_accuracy: 0.9624 - lr: 7.5000e-06\n",
      "Epoch 62/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9975\n",
      "Epoch 62: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 9s 113ms/step - loss: 0.0740 - accuracy: 0.9975 - val_loss: 0.1655 - val_accuracy: 0.9763 - lr: 7.5000e-06\n",
      "Epoch 63/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9979\n",
      "Epoch 63: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 8s 110ms/step - loss: 0.0718 - accuracy: 0.9979 - val_loss: 0.1921 - val_accuracy: 0.9654 - lr: 7.5000e-06\n",
      "Epoch 64/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9987\n",
      "Epoch 64: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 9s 113ms/step - loss: 0.0699 - accuracy: 0.9987 - val_loss: 0.1884 - val_accuracy: 0.9683 - lr: 2.2500e-06\n",
      "Epoch 65/65\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9975\n",
      "Epoch 65: val_loss did not improve from 0.16067\n",
      "74/74 [==============================] - 9s 111ms/step - loss: 0.0735 - accuracy: 0.9975 - val_loss: 0.1921 - val_accuracy: 0.9674 - lr: 2.2500e-06\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T19:51:59.680306Z",
     "start_time": "2024-07-29T19:51:58.667598Z"
    }
   },
   "cell_type": "code",
   "source": "model_1.evaluate(test_datagen)",
   "id": "7accf2968e67a759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 1s 35ms/step - loss: 0.4736 - accuracy: 0.8923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.47362667322158813, 0.892307698726654]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-29T18:29:20.860931Z",
     "start_time": "2024-07-29T18:29:20.592463Z"
    }
   },
   "cell_type": "code",
   "source": "resnet = tf.keras.models.load_model(\"/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/ML_Models/Trained_Models/Resnet\")\n",
   "id": "cc8e6b2a803e9d63",
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at /home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/ML_Models/Trained_Models/Resnet",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[38], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m resnet \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/ML_Models/Trained_Models/Resnet\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/saving/saving_api.py:262\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001B[0m\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m saving_lib\u001B[38;5;241m.\u001B[39mload_model(\n\u001B[1;32m    255\u001B[0m         filepath,\n\u001B[1;32m    256\u001B[0m         custom_objects\u001B[38;5;241m=\u001B[39mcustom_objects,\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;28mcompile\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mcompile\u001B[39m,\n\u001B[1;32m    258\u001B[0m         safe_mode\u001B[38;5;241m=\u001B[39msafe_mode,\n\u001B[1;32m    259\u001B[0m     )\n\u001B[1;32m    261\u001B[0m \u001B[38;5;66;03m# Legacy case.\u001B[39;00m\n\u001B[0;32m--> 262\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlegacy_sm_saving_lib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcustom_objects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcustom_objects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mcompile\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[0;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[0;32m~/PycharmProjects/eye_detection_fundus_dataset/.venv/lib/python3.10/site-packages/keras/src/saving/legacy/save.py:234\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(filepath, custom_objects, compile, options)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(filepath_str, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39mexists(filepath_str):\n\u001B[0;32m--> 234\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(\n\u001B[1;32m    235\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo file or directory found at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath_str\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    236\u001B[0m         )\n\u001B[1;32m    238\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mio\u001B[38;5;241m.\u001B[39mgfile\u001B[38;5;241m.\u001B[39misdir(filepath_str):\n\u001B[1;32m    239\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m saved_model_load\u001B[38;5;241m.\u001B[39mload(\n\u001B[1;32m    240\u001B[0m             filepath_str, \u001B[38;5;28mcompile\u001B[39m, options\n\u001B[1;32m    241\u001B[0m         )\n",
      "\u001B[0;31mOSError\u001B[0m: No file or directory found at /home/thefilthysalad/PycharmProjects/eye_detection_fundus_dataset/ML_Models/Trained_Models/Resnet"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "resnet.evaluate(val_datagen)",
   "id": "ebb33e0640d23929",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
